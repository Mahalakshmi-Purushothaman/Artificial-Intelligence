{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77d74627-a60d-4ddd-9631-7624609a3003",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'transaction_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, confusion_matrix\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Example transaction data with features like amount, location, etc.\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransaction_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Preprocessing: selecting relevant features\u001b[39;00m\n\u001b[0;32m     13\u001b[0m features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamount\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransaction_type\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_of_day\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'transaction_data.csv'"
     ]
    }
   ],
   "source": [
    "# Fraud Detection in Banking: In banking, fraud detection is a major use case for AI. Here, we can use a simple algorithm \n",
    "# to analyze transaction data and detect potential fraudulent activities based on historical patterns.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Example transaction data with features like amount, location, etc.\n",
    "data = pd.read_csv('transaction_data.csv')  # Load dataset\n",
    "\n",
    "# Preprocessing: selecting relevant features\n",
    "features = ['amount', 'transaction_type', 'location', 'time_of_day']\n",
    "X = data[features]\n",
    "y = data['fraudulent']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "750d458b-9fa0-4e3a-bbf9-ce72ba595d37",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'loan_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Example loan data (loan_amount, income, credit_score, etc.)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloan_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Features and target variable (default: 0 = no, 1 = yes)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m X \u001b[38;5;241m=\u001b[39m data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloan_amount\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincome\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcredit_score\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloan_term\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'loan_data.csv'"
     ]
    }
   ],
   "source": [
    "# Loan Default Prediction: In the banking sector, predicting whether a loan applicant will default on a loan can be done \n",
    "# using machine learning algorithms, which analyze historical data to make predictions.\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Example loan data (loan_amount, income, credit_score, etc.)\n",
    "data = pd.read_csv('loan_data.csv')\n",
    "\n",
    "# Features and target variable (default: 0 = no, 1 = yes)\n",
    "X = data[['loan_amount', 'income', 'credit_score', 'loan_term']]\n",
    "y = data['default']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a decision tree classifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict loan defaults on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b57e8b-7a1e-4627-bb07-21864b9baf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer Segmentation: In banking, deep learning can segment customers based on their spending patterns, \n",
    "# helping banks target customers with personalized offers.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load customer data\n",
    "data = pd.read_csv('customer_data.csv')\n",
    "\n",
    "# Features: spending habits, age, income, etc.\n",
    "X = data[['age', 'income', 'monthly_spend']]\n",
    "y = data['customer_segment']  # Target: customer segment (e.g., High Value, Low Value)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define a neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=3))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))  # Binary classification: High Value or Low Value\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_scaled, y, epochs=10, batch_size=32)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_scaled)\n",
    "\n",
    "# Evaluate model accuracy\n",
    "accuracy = model.evaluate(X_scaled, y)\n",
    "print(f\"Model Accuracy: {accuracy[1] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295f25ce-d7b2-4d7b-bda8-fc85a4ece0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loan Approval Prediction Using Random Forest (Supervised):\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Loan approval data\n",
    "data = pd.read_csv('loan_approval.csv')\n",
    "\n",
    "# Features and target variable\n",
    "X = data[['age', 'income', 'loan_amount', 'credit_score']]\n",
    "y = data['loan_approved']  # 0 = No, 1 = Yes\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a random forest model\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f90f68-78fd-44eb-9a72-130d5c3610ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly Detection Using K-Means Clustering (Unsupervised):\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Transaction data (e.g., amount, frequency, location)\n",
    "data = pd.read_csv('transaction_data.csv')\n",
    "\n",
    "# Features: transaction amount, transaction frequency, etc.\n",
    "X = data[['amount', 'frequency', 'location']]\n",
    "\n",
    "# Apply K-Means clustering to detect anomalies\n",
    "kmeans = KMeans(n_clusters=2)  # 2 clusters: Normal vs. Fraudulent\n",
    "data['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Plot clusters\n",
    "plt.scatter(data['amount'], data['frequency'], c=data['cluster'])\n",
    "plt.xlabel('Amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Transaction Clusters: Normal vs. Fraudulent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96bb425-dfa3-43d1-a208-109c0dc4c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting Unusual Login Behavior (Anomaly Detection):\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import pandas as pd\n",
    "\n",
    "# Customer login data (time of login, IP address, device used)\n",
    "data = pd.read_csv('login_data.csv')\n",
    "\n",
    "# Features: time of login, IP address (encoded), device type\n",
    "X = data[['login_time', 'ip_address', 'device_type']]\n",
    "\n",
    "# Use Isolation Forest for anomaly detection (outlier detection)\n",
    "model = IsolationForest(n_estimators=100, contamination=0.05)\n",
    "data['anomaly'] = model.fit_predict(X)\n",
    "\n",
    "# Print results (1 = normal, -1 = anomaly)\n",
    "print(data[data['anomaly'] == -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768429ef-f6e9-43f4-89e8-1d693f5211ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging (Random Forest): Scenario: Predicting loan approval based on historical customer data (e.g., credit score, income, age, etc.).\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generating a synthetic banking dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Random Forest (Bagging)\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91525ee-2bad-4eba-a697-3369f7dea6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting (Gradient Boosting): Scenario: Predicting whether a customer will default on a loan based on various features \n",
    "# such as payment history, loan type, etc.\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generating a synthetic banking dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Gradient Boosting Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27913c5a-6b39-48a6-a3ac-cd879a5ae5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking (Stacked Generalization): Scenario: Predicting customer churn in a bank based on customer interaction, account activity, \n",
    "# and demographic data.\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generating a synthetic banking dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Defining base models\n",
    "base_models = [\n",
    "    ('lr', LogisticRegression(random_state=42)),\n",
    "    ('svm', SVC(random_state=42)),\n",
    "    ('dt', DecisionTreeClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "# StackingClassifier\n",
    "stacking_model = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression())\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = stacking_model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Stacking Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d21ba1-c989-4bc4-add1-5d6318561240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting (VotingClassifier): Scenario: Classifying customers into segments based on financial behavior (e.g., high-value, medium-value, low-value).\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generating a synthetic banking dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the models\n",
    "model1 = LogisticRegression(random_state=42)\n",
    "model2 = SVC(random_state=42)\n",
    "model3 = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Voting Classifier (Hard Voting)\n",
    "voting_model = VotingClassifier(estimators=[('lr', model1), ('svm', model2), ('dt', model3)], voting='hard')\n",
    "voting_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = voting_model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Voting Classifier Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a08d3b4-5e41-48f0-891c-2b1df2ff0100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Mixture Model (GMM): Scenario: Predicting customer behavior for segmentation, such as differentiating between \n",
    "# high-value and low-value customers based on their spending patterns and income.\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generating a synthetic dataset for customer spending patterns\n",
    "X, _ = make_blobs(n_samples=500, centers=2, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Fit a Gaussian Mixture Model with 2 components (clusters)\n",
    "gmm = GaussianMixture(n_components=2)\n",
    "gmm.fit(X)\n",
    "\n",
    "# Generate new synthetic data points based on the learned distribution\n",
    "new_samples, _ = gmm.sample(5)\n",
    "\n",
    "# Visualize the clusters and generated data\n",
    "plt.scatter(X[:, 0], X[:, 1], c='blue', label='Existing Customers')\n",
    "plt.scatter(new_samples[:, 0], new_samples[:, 1], c='red', label='Generated Customers')\n",
    "plt.legend()\n",
    "plt.title(\"Gaussian Mixture Model - Customer Segmentation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efba4ae-c15a-4e1c-966e-532f3306c55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Markov Model (HMM): Scenario: Predicting whether a customer will continue using a bank's services (churn prediction) \n",
    "# based on historical transactional data over time.\n",
    "\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "import numpy as np\n",
    "\n",
    "# Synthetic data representing customer transaction behavior over time\n",
    "# Let's assume 2 hidden states: 0 (Inactive) and 1 (Active)\n",
    "X = np.array([[0.1], [0.2], [0.3], [0.4], [1.5], [1.6], [1.7], [0.5]])\n",
    "\n",
    "# Fit a Hidden Markov Model with 2 states\n",
    "model = GaussianHMM(n_components=2, covariance_type=\"diag\", n_iter=1000)\n",
    "model.fit(X)\n",
    "\n",
    "# Predict the hidden states for the data\n",
    "hidden_states = model.predict(X)\n",
    "\n",
    "print(\"Predicted States:\", hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fa80ee-a73c-41f1-9869-a07dce0e1ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variational Autoencoder (VAE): Scenario: Generating new synthetic data for customer profiles (e.g., income, age,\n",
    "# spending habits) for simulation or testing purposes in a bank.\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Generating synthetic customer data (income, age, etc.)\n",
    "X = np.random.rand(1000, 2)  # Synthetic data for two features\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# VAE model\n",
    "latent_dim = 2  # Latent space dimension\n",
    "\n",
    "inputs = layers.Input(shape=(2,))\n",
    "h = layers.Dense(64, activation='relu')(inputs)\n",
    "z_mean = layers.Dense(latent_dim)(h)\n",
    "z_log_var = layers.Dense(latent_dim)(h)\n",
    "\n",
    "z = layers.Lambda(lambda args: args[0] + tf.exp(args[1] / 2) * tf.random.normal(tf.shape(args[0])))([z_mean, z_log_var])\n",
    "\n",
    "encoder = models.Model(inputs, [z_mean, z_log_var, z])\n",
    "\n",
    "# Decoder\n",
    "latent_inputs = layers.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(64, activation='relu')(latent_inputs)\n",
    "decoded = layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "decoder = models.Model(latent_inputs, decoded)\n",
    "\n",
    "# VAE model (Encoder + Decoder)\n",
    "vae = models.Model(inputs, decoder(encoder(inputs)[2]))\n",
    "\n",
    "# Define the loss\n",
    "xent_loss = 2 * tf.reduce_sum(X_scaled * tf.log(X_scaled / vae(inputs)) + (1 - X_scaled) * tf.log((1 - X_scaled) / (1 - vae(inputs))), axis=-1)\n",
    "kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "\n",
    "vae_loss = tf.reduce_mean(xent_loss + kl_loss)\n",
    "\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "\n",
    "# Train the model (for simplicity, using the same data for training)\n",
    "vae.fit(X_scaled, epochs=10, batch_size=32)\n",
    "\n",
    "# Generate new synthetic data (sample from latent space)\n",
    "new_data = np.random.normal(size=(5, latent_dim))\n",
    "generated_data = decoder.predict(new_data)\n",
    "\n",
    "print(\"Generated Synthetic Data:\")\n",
    "print(generated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417788cd-3358-4169-9b62-5f694933d217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative Adversarial Network (GAN): Scenario: Generating synthetic customer transaction data to simulate realistic patterns \n",
    "# for training predictive models (e.g., fraud detection).\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# GAN Generator Model\n",
    "def build_generator(latent_dim):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, activation='relu', input_dim=latent_dim))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dense(2, activation='sigmoid'))  # Output for 2 features\n",
    "    return model\n",
    "\n",
    "# GAN Discriminator Model\n",
    "def build_discriminator():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(512, activation='relu', input_dim=2))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))  # Binary classification: real/fake\n",
    "    return model\n",
    "\n",
    "# GAN Model (Combining Generator and Discriminator)\n",
    "latent_dim = 10\n",
    "generator = build_generator(latent_dim)\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Discriminator model\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# GAN model\n",
    "discriminator.trainable = False\n",
    "gan_input = layers.Input(shape=(latent_dim,))\n",
    "x = generator(gan_input)\n",
    "gan_output = discriminator(x)\n",
    "\n",
    "gan = models.Model(gan_input, gan_output)\n",
    "gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# Training GAN\n",
    "def train_gan(epochs=1000, batch_size=128):\n",
    "    for epoch in range(epochs):\n",
    "        # Train Discriminator\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        generated_data = generator.predict(noise)\n",
    "        \n",
    "        real_data = np.random.rand(batch_size, 2)  # Synthetic real data for illustration\n",
    "        \n",
    "        X = np.concatenate([real_data, generated_data])\n",
    "        y = np.concatenate([np.ones(batch_size), np.zeros(batch_size)])\n",
    "        \n",
    "        d_loss, d_acc = discriminator.train_on_batch(X, y)\n",
    "        \n",
    "        # Train Generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        y_gen = np.ones(batch_size)  # Generator aims to fool the discriminator\n",
    "        \n",
    "        g_loss = gan.train_on_batch(noise, y_gen)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}: Discriminator Loss = {d_loss}, Generator Loss = {g_loss}\")\n",
    "\n",
    "train_gan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b0fab7-5496-4a4f-89aa-9fd123621897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative Adversarial Networks (GANs): Scenario: Generating synthetic fraudulent transaction data to train fraud detection models.\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# GAN Generator Model\n",
    "def build_generator(latent_dim):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, activation='relu', input_dim=latent_dim))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dense(2, activation='sigmoid'))  # Output for 2 features (e.g., amount, transaction type)\n",
    "    return model\n",
    "\n",
    "# GAN Discriminator Model\n",
    "def build_discriminator():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(512, activation='relu', input_dim=2))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))  # Binary classification: real/fake\n",
    "    return model\n",
    "\n",
    "# GAN Model (Combining Generator and Discriminator)\n",
    "latent_dim = 10\n",
    "generator = build_generator(latent_dim)\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Discriminator model\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# GAN model\n",
    "discriminator.trainable = False\n",
    "gan_input = layers.Input(shape=(latent_dim,))\n",
    "x = generator(gan_input)\n",
    "gan_output = discriminator(x)\n",
    "\n",
    "gan = models.Model(gan_input, gan_output)\n",
    "gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# Training GAN\n",
    "def train_gan(epochs=1000, batch_size=128):\n",
    "    for epoch in range(epochs):\n",
    "        # Train Discriminator\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        generated_data = generator.predict(noise)\n",
    "        \n",
    "        real_data = np.random.rand(batch_size, 2)  # Synthetic real data for illustration\n",
    "        \n",
    "        X = np.concatenate([real_data, generated_data])\n",
    "        y = np.concatenate([np.ones(batch_size), np.zeros(batch_size)])\n",
    "        \n",
    "        d_loss, d_acc = discriminator.train_on_batch(X, y)\n",
    "        \n",
    "        # Train Generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "        y_gen = np.ones(batch_size)  # Generator aims to fool the discriminator\n",
    "        \n",
    "        g_loss = gan.train_on_batch(noise, y_gen)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}: Discriminator Loss = {d_loss}, Generator Loss = {g_loss}\")\n",
    "\n",
    "train_gan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb72d7-c47f-4c8d-9bcc-bf72798a0e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial Attacks and Robustness: Scenario: Generating adversarial examples to test the robustness of a fraud detection model.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generating synthetic data (transaction amount and type for simplicity)\n",
    "X = np.random.rand(1000, 2)  # Features: [transaction amount, transaction type]\n",
    "y = (X[:, 0] > 0.5).astype(int)  # Fraud = 1 if amount > 0.5, else not fraud = 0\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a simple SVM model\n",
    "model = SVC(kernel='linear', probability=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Generate adversarial example: small perturbation to fool the model\n",
    "X_adv = X_test + np.random.normal(0, 0.1, X_test.shape)  # Add noise to create adversarial examples\n",
    "\n",
    "# Predict using the original and adversarial test data\n",
    "y_pred_original = model.predict(X_test)\n",
    "y_pred_adv = model.predict(X_adv)\n",
    "\n",
    "# Calculate accuracy\n",
    "acc_original = accuracy_score(y_test, y_pred_original)\n",
    "acc_adv = accuracy_score(y_test, y_pred_adv)\n",
    "\n",
    "print(f\"Accuracy on Original Test Set: {acc_original}\")\n",
    "print(f\"Accuracy on Adversarial Test Set: {acc_adv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ca21e4-f366-424c-8a0a-450ac9bed5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial Training: Scenario: Incorporating adversarial examples into the training process to improve the robustness of a fraud detection model.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generating synthetic data (transaction amount and type)\n",
    "X = np.random.rand(1000, 2)\n",
    "y = (X[:, 0] > 0.5).astype(int)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Adversarial training function\n",
    "def adversarial_training(model, X_train, y_train, X_test, y_test, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        # Train on the original data\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Generate adversarial examples\n",
    "        X_adv = X_train + np.random.normal(0, 0.1, X_train.shape)\n",
    "        \n",
    "        # Train on adversarial examples as well\n",
    "        model.fit(X_adv, y_train)\n",
    "        \n",
    "        # Evaluate the model on both the original and adversarial test set\n",
    "        y_pred_original = model.predict(X_test)\n",
    "        y_pred_adv = model.predict(X_test + np.random.normal(0, 0.1, X_test.shape))\n",
    "        \n",
    "        acc_original = accuracy_score(y_test, y_pred_original)\n",
    "        acc_adv = accuracy_score(y_test, y_pred_adv)\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Accuracy on Original Test Set = {acc_original}, Accuracy on Adversarial Test Set = {acc_adv}\")\n",
    "\n",
    "# Train a simple SVM model\n",
    "model = SVC(kernel='linear')\n",
    "\n",
    "# Adversarial training\n",
    "adversarial_training(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa742d3b-889a-436f-b0a6-64f8864e1e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial Networks for Reinforcement Learning: Scenario: Training a reinforcement learning agent to handle adversarial economic \n",
    "# conditions that could simulate fraudulent activities.\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Create a custom environment for financial decision-making\n",
    "class BankEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(BankEnv, self).__init__()\n",
    "        self.action_space = gym.spaces.Discrete(3)  # 3 actions: approve loan, deny loan, flag for review\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(5,), dtype=np.float32)  # Random economic conditions\n",
    "\n",
    "    def reset(self):\n",
    "        return np.random.rand(5)\n",
    "\n",
    "    def step(self, action):\n",
    "        state = np.random.rand(5)\n",
    "        reward = -1 if action == 2 else 1  # Deny flag results in penalty, else reward\n",
    "        done = False\n",
    "        return state, reward, done, {}\n",
    "\n",
    "# Initialize the custom environment\n",
    "env = BankEnv()\n",
    "\n",
    "# Initialize the agent (Proximal Policy Optimization - PPO)\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Simulate an adversarial environment by introducing adversarial economic conditions\n",
    "adversarial_conditions = np.random.rand(5) * 0.5  # Example of adversarial changes\n",
    "\n",
    "# Test the agent with adversarial conditions\n",
    "state = adversarial_conditions\n",
    "action = model.predict(state)[0]\n",
    "\n",
    "print(f\"Adversarial Action: {action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83d49a9-807d-4157-8b82-4d9557747831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collaborative Filtering (User-Item Based): Scenario: Recommending financial products (like loans, credit cards) based on\n",
    "# the preferences of similar users.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Example transaction data (user_id, product_id)\n",
    "data = {\n",
    "    'user_id': [1, 2, 3, 4, 5, 6],\n",
    "    'loan': [1, 0, 1, 0, 1, 0],\n",
    "    'credit_card': [0, 1, 1, 0, 1, 1],\n",
    "    'mortgage': [1, 1, 0, 0, 1, 0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Model: Nearest Neighbors (Collaborative Filtering)\n",
    "X = df.drop(columns=['user_id'])  # Features: transaction data\n",
    "\n",
    "# Fit Nearest Neighbors model\n",
    "model = NearestNeighbors(n_neighbors=2, metric='cosine')\n",
    "model.fit(X)\n",
    "\n",
    "# Find similar users to user 1\n",
    "distances, indices = model.kneighbors([X.iloc[0]])\n",
    "\n",
    "# Recommend products from similar users\n",
    "recommended_products = X.iloc[indices[0]].sum(axis=0).sort_values(ascending=False).index[0:2]  # Top 2 recommended products\n",
    "print(\"Recommended products for User 1:\", recommended_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee3c36c-2aa6-4696-a457-4a0c15de719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content-Based Filtering: Scenario: Recommending a financial product (like a credit card) based on the features of the \n",
    "# product and users past behavior (e.g., transaction data).\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Example data (product_id, product description)\n",
    "product_data = {\n",
    "    'product_id': [1, 2, 3, 4],\n",
    "    'product_description': ['Gold credit card', 'Platinum credit card', 'Student credit card', 'Cashback credit card'],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(product_data)\n",
    "\n",
    "# User's past interaction (the user prefers Gold and Platinum credit cards)\n",
    "user_preferences = ['Gold credit card', 'Platinum credit card']\n",
    "\n",
    "# Convert text to vector form\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(df['product_description'])\n",
    "\n",
    "# User's preference vector\n",
    "user_vector = vectorizer.transform(user_preferences)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity_scores = cosine_similarity(user_vector, tfidf_matrix)\n",
    "\n",
    "# Get the top recommended product\n",
    "top_product_index = similarity_scores.argmax()\n",
    "recommended_product = df.iloc[top_product_index]['product_description']\n",
    "print(\"Recommended product:\", recommended_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fe48c6-bb1a-47c0-95f8-fb3053168eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Models: Scenario: Recommending financial products by combining customer preferences (content-based) with their \n",
    "# interactions (collaborative filtering).\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np\n",
    "\n",
    "# Example user-item interaction matrix (user, loan, credit card, mortgage)\n",
    "data = {\n",
    "    'user_id': [1, 2, 3, 4, 5],\n",
    "    'loan': [1, 0, 1, 0, 1],\n",
    "    'credit_card': [0, 1, 1, 0, 1],\n",
    "    'mortgage': [1, 1, 0, 0, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Content-based: Item features (e.g., product categories)\n",
    "item_features = np.array([[1, 0, 0],  # Loan\n",
    "                          [0, 1, 0],  # Credit Card\n",
    "                          [0, 0, 1]]) # Mortgage\n",
    "\n",
    "# Collaborative-based: User-item interaction matrix (e.g., loan, credit card, mortgage interactions)\n",
    "user_item_matrix = df.drop(columns=['user_id']).values\n",
    "\n",
    "# Hybrid model: Combine both\n",
    "user_distances = pairwise_distances(user_item_matrix, metric='cosine')\n",
    "item_distances = pairwise_distances(item_features, metric='cosine')\n",
    "\n",
    "# Combine collaborative and content-based distances\n",
    "combined_distances = user_distances + item_distances.T\n",
    "\n",
    "# Recommend products for user 1\n",
    "recommended_product_idx = np.argmin(combined_distances[0])\n",
    "recommended_product = df.columns[recommended_product_idx + 1]  # Skip user_id column\n",
    "print(f\"Recommended product for User 1: {recommended_product}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e616ddff-6521-4a40-b0c8-4ea205b8987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Factorization (e.g., SVD): Scenario: Predicting which financial product a user is likely to apply for based on past interactions.\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Example user-item interaction matrix (user, loan, credit card, mortgage)\n",
    "data = {\n",
    "    'user_id': [1, 2, 3, 4, 5],\n",
    "    'loan': [1, 0, 1, 0, 1],\n",
    "    'credit_card': [0, 1, 1, 0, 1],\n",
    "    'mortgage': [1, 1, 0, 0, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Matrix factorization: Apply Singular Value Decomposition (SVD)\n",
    "X = df.drop(columns=['user_id']).values\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "X_svd = svd.fit_transform(X)\n",
    "\n",
    "# Predict missing values (which product a user is likely to apply for)\n",
    "predicted_ratings = np.dot(X_svd, svd.components_)\n",
    "\n",
    "# Predict for User 1 (which product they might apply for)\n",
    "user_1_predictions = predicted_ratings[0]\n",
    "recommended_product_idx = np.argmax(user_1_predictions)\n",
    "recommended_product = df.columns[recommended_product_idx + 1]  # Skip user_id column\n",
    "print(f\"Recommended product for User 1: {recommended_product}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aa729f-b825-4110-a0cc-b84b99b5dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Association Rule Learning (Apriori Algorithm): Scenario: Recommending related financial products based on past user \n",
    "# transactions (e.g., credit card after loan approval).\n",
    "\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import pandas as pd\n",
    "\n",
    "# Example user-product transaction data\n",
    "data = {\n",
    "    'loan': [1, 0, 1, 1, 0],\n",
    "    'credit_card': [0, 1, 1, 1, 0],\n",
    "    'mortgage': [1, 0, 0, 1, 0],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply Apriori algorithm to find frequent itemsets\n",
    "frequent_itemsets = apriori(df, min_support=0.2, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric='lift', min_threshold=1.0)\n",
    "\n",
    "# Display recommendations based on rules\n",
    "print(rules[['antecedents', 'consequents', 'lift']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607b4b4a-96dc-4448-bebb-1a35890a4764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Example for User-Based Collaborative Filtering: \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Example user-item matrix (user, loan, credit card, mortgage)\n",
    "data = {\n",
    "    'user_id': [1, 2, 3, 4, 5],\n",
    "    'loan': [1, 0, 1, 0, 1],\n",
    "    'credit_card': [0, 1, 1, 0, 1],\n",
    "    'mortgage': [1, 1, 0, 0, 1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Extract the features (excluding user_id)\n",
    "X = df.drop(columns=['user_id'])\n",
    "\n",
    "# Build the model: Nearest Neighbors (User-based CF)\n",
    "model = NearestNeighbors(n_neighbors=3, metric='cosine')\n",
    "model.fit(X)\n",
    "\n",
    "# Find the 3 most similar users to user 1\n",
    "distances, indices = model.kneighbors([X.iloc[0]])\n",
    "\n",
    "# Print similar users and the products they are likely to be interested in\n",
    "similar_users = X.iloc[indices[0]]\n",
    "recommended_products = similar_users.sum(axis=0).sort_values(ascending=False).index[:2]  # Top 2 recommended products\n",
    "print(\"Recommended products for User 1 based on similar users:\", recommended_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844d7bf5-ba6d-4c88-b23d-04cf567b8e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Example for Item-Based Collaborative Filtering\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Example user-item matrix (user, loan, credit card, mortgage)\n",
    "data = {\n",
    "    'user_id': [1, 2, 3, 4, 5],\n",
    "    'loan': [1, 0, 1, 0, 1],\n",
    "    'credit_card': [0, 1, 1, 0, 1],\n",
    "    'mortgage': [1, 1, 0, 0, 1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Extract the user-item interaction matrix\n",
    "X = df.drop(columns=['user_id'])\n",
    "\n",
    "# Calculate cosine similarity between items\n",
    "item_similarity = cosine_similarity(X.T)  # Transpose so that items are compared\n",
    "\n",
    "# Recommend similar products to 'loan'\n",
    "loan_idx = df.columns.get_loc('loan')  # Get the index of 'loan' column\n",
    "similar_items = item_similarity[loan_idx]\n",
    "\n",
    "# Print the top 2 most similar items to 'loan'\n",
    "recommended_items = df.columns[similar_items.argsort()[-3:][::-1]]  # Top 2 recommended items\n",
    "print(\"Items similar to 'loan' that might be recommended:\", recommended_items[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e174b1-b3f4-465e-b34c-af73c1e28683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Example for Matrix Factorization using SVD\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "\n",
    "# Example user-item matrix (user, loan, credit card, mortgage)\n",
    "data = {\n",
    "    'user_id': [1, 2, 3, 4, 5],\n",
    "    'loan': [1, 0, 1, 0, 1],\n",
    "    'credit_card': [0, 1, 1, 0, 1],\n",
    "    'mortgage': [1, 1, 0, 0, 1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Extract the user-item matrix (exclude user_id column)\n",
    "X = df.drop(columns=['user_id']).values\n",
    "\n",
    "# Apply Singular Value Decomposition (SVD)\n",
    "svd = TruncatedSVD(n_components=2)  # Reduce the matrix to 2 components\n",
    "X_svd = svd.fit_transform(X)\n",
    "\n",
    "# Predict missing values (which product a user is likely to apply for)\n",
    "predicted_ratings = np.dot(X_svd, svd.components_)\n",
    "\n",
    "# Predict for User 1 (which product they might apply for)\n",
    "user_1_predictions = predicted_ratings[0]\n",
    "recommended_product_idx = np.argmax(user_1_predictions)\n",
    "recommended_product = df.columns[recommended_product_idx + 1]  # Skip user_id column\n",
    "print(f\"Recommended product for User 1 based on SVD: {recommended_product}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cd680e-4a89-41fd-b27d-07df96d44451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code (GPT-3 with OpenAI API):\n",
    "\n",
    "import openai\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"your-api-key\"\n",
    "\n",
    "def generate_response(prompt):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",  # GPT-3 model\n",
    "        prompt=prompt,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Example banking query\n",
    "prompt = \"I want to check the balance in my savings account.\"\n",
    "response = generate_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c156c1-0310-486a-b017-1504292225b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code (BERT for Sentiment Analysis using HuggingFace's Transformers):\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load sentiment-analysis pipeline from Hugging Face\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Example feedback\n",
    "feedback = \"The customer service at your branch was excellent!\"\n",
    "\n",
    "# Perform sentiment analysis\n",
    "result = sentiment_analysis(feedback)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2e2eb6-3975-440e-8214-f2a1cc02915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code (BERT for Masked Language Modeling):\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load masked language model\n",
    "fill_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "\n",
    "# Example with a masked token\n",
    "text = \"I want to withdraw money from my [MASK] account.\"\n",
    "result = fill_mask(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c4a07c-6519-40a8-a69e-abd78e56b8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code (T5 for Summarization using HuggingFace's Transformers):\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load T5 model for summarization\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-base\")\n",
    "\n",
    "# Example transaction history\n",
    "text = \"\"\"\n",
    "Customer John Doe made a series of transactions in December 2024. On December 1st, he deposited $2000 in his savings account. \n",
    "On December 5th, he withdrew $500 from the same account. Then, on December 10th, he transferred $1000 to his checking account.\n",
    "\"\"\"\n",
    "\n",
    "# Perform summarization\n",
    "summary = summarizer(text)\n",
    "print(summary[0]['summary_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89248ee7-8b50-4a65-879f-8c9e23b822bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code (Example with Vision and Text):\n",
    "\n",
    "# This is a placeholder code since actual multimodal models require integration with computer vision libraries.\n",
    "from transformers import VisionTextDualEncoderProcessor, VisionTextDualEncoderModel\n",
    "\n",
    "# Example: This code is for illustration. In practice, we'd use image-to-text models to extract data from checks.\n",
    "processor = VisionTextDualEncoderProcessor.from_pretrained('model_name')\n",
    "model = VisionTextDualEncoderModel.from_pretrained('model_name')\n",
    "\n",
    "# This step requires an image input, which isn't feasible to demonstrate in code here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd87a34-28a1-4a0d-98f3-f1620a996827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Example for Tokenization and Preprocessing:\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "import re\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Sample banking-related text\n",
    "text = \"I want to transfer $500 to my savings account. Can you help me?\"\n",
    "\n",
    "# Preprocess the text (e.g., remove special characters)\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove non-alphanumeric characters\n",
    "    return text\n",
    "\n",
    "processed_text = preprocess_text(text)\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer(processed_text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32654c9c-8734-41c6-844f-41abff62c867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Example for Pretraining GPT-like Model:\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Example training dataset\n",
    "train_data = [\"Bank account balance check\", \"Transfer funds to checking account\", \"Apply for a home loan\"]\n",
    "\n",
    "# Tokenize the training data\n",
    "train_encodings = tokenizer(train_data, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    num_train_epochs=1,              # Number of training epochs\n",
    "    per_device_train_batch_size=4,   # Batch size\n",
    "    logging_dir='./logs',            # Log directory\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_encodings\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36696615-64ce-4070-9224-df791ba537c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Example for Fine-tuning BERT for Text Classification:\n",
    "\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a dataset (for example, a dataset of customer complaints)\n",
    "dataset = load_dataset(\"banking77\")  # Hypothetical dataset related to banking services\n",
    "\n",
    "# Load a pretrained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)  # 5 classes, e.g., loan, transfer, balance, etc.\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    evaluation_strategy=\"epoch\",     # Evaluate after each epoch\n",
    "    per_device_train_batch_size=16,  # Adjust depending on your GPU memory\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,              # Number of epochs\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test']\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30691195-c086-4627-a92e-725b045229b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Example for Model Evaluation:\n",
    "\n",
    "# Evaluate the trained model on the validation dataset\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Print evaluation results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe6088e-ae68-4e17-8f5b-71acaa49d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Example for Making Predictions:\n",
    "\n",
    "# Making a prediction with the fine-tuned model\n",
    "input_text = \"How do I transfer funds to another account?\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Predict the response\n",
    "outputs = model.generate(inputs['input_ids'])\n",
    "\n",
    "# Decode the output tokens\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9774887b-8910-4bc5-8a07-3c7295dde376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code: Tokenization with BERT\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example banking sentence\n",
    "sentence = \"Can you check my account balance?\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "# Display tokenized input\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589768fb-e5e5-4f55-ac5b-2a4bc4c65989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code: Attention in GPT (Text Generation)\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 tokenizer and model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Encode the prompt\n",
    "prompt = \"I want to transfer $500 to my savings account.\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "# Get the models prediction (i.e., the next word prediction)\n",
    "outputs = model.generate(inputs['input_ids'], max_length=50)\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ecc76b-4796-4178-9022-9b083dd06994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code: Using BERT for Sentence Classification (e.g., Sentiment Analysis)\n",
    "\n",
    "from transformers import BertForSequenceClassification, pipeline\n",
    "\n",
    "# Load a pre-trained model and tokenizer for sentiment analysis\n",
    "sentiment_analysis = pipeline('sentiment-analysis', model='bert-base-uncased')\n",
    "\n",
    "# Example customer feedback from a banking service\n",
    "feedback = \"I am very happy with the customer service at your bank!\"\n",
    "\n",
    "# Perform sentiment analysis\n",
    "result = sentiment_analysis(feedback)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68d6fb5-0f2a-4ae5-a412-fc8c2683f9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code: Fine-tuning BERT for Banking Classification Task\n",
    "\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a sample banking dataset (hypothetical example)\n",
    "dataset = load_dataset('banking77')  # A hypothetical dataset for banking-related tasks\n",
    "\n",
    "# Load a pre-trained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "\n",
    "# Tokenize the dataset\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    evaluation_strategy=\"epoch\",     # Evaluate after each epoch\n",
    "    per_device_train_batch_size=16,  # Batch size\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,              # Number of epochs\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test']\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003a112c-6dff-4cd6-a73c-ff2b32c3be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code: Text Generation Using GPT-2\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Prompt for banking scenario\n",
    "prompt = \"How can I apply for a home loan?\"\n",
    "\n",
    "# Tokenize input and generate response\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "outputs = model.generate(inputs['input_ids'], max_length=100)\n",
    "\n",
    "# Decode the generated response\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0800d08-178b-4cab-88a9-3635cfb896f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Python Code Sample (Text Generation using GPT-2):\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Sample prompt for the banking scenario\n",
    "prompt = \"What are the requirements to open a savings account at your bank?\"\n",
    "\n",
    "# Tokenize the input prompt\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "# Generate a response (text completion)\n",
    "outputs = model.generate(inputs['input_ids'], max_length=100)\n",
    "\n",
    "# Decode and print the generated response\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107cccf5-1431-49a5-84de-010133529c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Python Code Sample (Sentiment Analysis using BERT):\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained BERT model for sentiment analysis\n",
    "sentiment_analysis = pipeline('sentiment-analysis', model='bert-base-uncased')\n",
    "\n",
    "# Example customer feedback for sentiment analysis\n",
    "feedback = \"The loan application process was quick and easy. Great service!\"\n",
    "\n",
    "# Perform sentiment analysis\n",
    "result = sentiment_analysis(feedback)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bcd7da-b737-41bb-b1e5-f8854b625fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Python Code Sample (NER using SpaCy):\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load pre-trained SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example banking sentence\n",
    "text = \"I transferred $500 to my account on 12/24/2024.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract named entities\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} ({ent.label_})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d526065-f75c-452a-b024-1391e0356417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Sample (Summarization using T5):\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load pre-trained T5 model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "# Example long text (banking-related)\n",
    "text = \"\"\"\n",
    "The bank offers a wide range of financial products including savings accounts, checking accounts, loans, mortgages, and investment options. \n",
    "Customers can open accounts online or at any of our local branches. \n",
    "We also provide digital banking services including mobile banking apps for easy account management. \n",
    "Our loan offerings include personal loans, business loans, and home loans.\n",
    "\"\"\"\n",
    "\n",
    "# Prepare the text for summarization\n",
    "inputs = tokenizer(\"summarize: \" + text, return_tensors='pt', max_length=512, truncation=True)\n",
    "\n",
    "# Generate the summary\n",
    "summary_ids = model.generate(inputs['input_ids'], max_length=100, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Decode the summary\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee422b73-6c37-407c-a482-f6f2089e3049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Sample (QA using BERT):\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained QA model\n",
    "qa_pipeline = pipeline('question-answering', model='bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "# Example context (banking-related document)\n",
    "context = \"\"\"\n",
    "The bank offers various types of loans, including personal loans, auto loans, and home loans. \n",
    "To apply for a loan, customers must submit proof of income, identification, and credit history.\n",
    "\"\"\"\n",
    "\n",
    "# Example question\n",
    "question = \"What do I need to apply for a loan?\"\n",
    "\n",
    "# Perform question answering\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11653b6-d0ef-43d0-b68c-ac47645b31f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Sample (STT using SpeechRecognition):\n",
    "\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Initialize recognizer\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Use the microphone as the audio source\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Listening for speech...\")\n",
    "    audio = recognizer.listen(source)\n",
    "\n",
    "# Convert speech to text\n",
    "text = recognizer.recognize_google(audio)\n",
    "print(f\"Recognized text: {text}\")\n",
    "# # Python Code Sample (TTS using pyttsx3):\n",
    "\n",
    "import pyttsx3\n",
    "\n",
    "# Initialize the text-to-speech engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Example response from a chatbot\n",
    "response = \"Your loan application has been successfully submitted.\"\n",
    "\n",
    "# Convert text to speech\n",
    "engine.say(response)\n",
    "engine.runAndWait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9157fc-1831-4148-98d7-5cdc46b1aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Sample (Translation using MarianMT):\n",
    "\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load pre-trained MarianMT model for translation\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-fr'  # English to French\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"How can I apply for a loan?\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Translate\n",
    "translated = model.generate(inputs['input_ids'])\n",
    "translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "print(translated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5c51bb-9bf9-4c6a-bbba-5818c2ffa71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Sample (Document Processing using Tesseract and NLP):\n",
    "\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "\n",
    "# Example document image (e.g., bank statement)\n",
    "image = Image.open('bank_statement.jpg')\n",
    "\n",
    "# Extract text from the image using Tesseract OCR\n",
    "text = pytesseract.image_to_string(image)\n",
    "\n",
    "# Use NLP model to analyze extracted text\n",
    "nlp = pipeline(\"ner\")\n",
    "ner_results = nlp(text)\n",
    "print(ner_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e3b06c-2a65-49f9-a305-1b6c4b27c243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Sample (Tokenization using BERT):\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"Can you check my account balance?\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "# Convert tokens to input IDs for BERT\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Display tokens and input IDs\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Input IDs:\", input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa18927-cc25-45d2-bc64-97cd29eb62fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Sample (Word Embeddings using BERT):\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"Can you check my account balance?\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# Get token embeddings from BERT\n",
    "outputs = model(**inputs)\n",
    "embeddings = outputs.last_hidden_state\n",
    "\n",
    "# Display embeddings of the first token\n",
    "print(embeddings[0][0])  # Embedding for the first token 'can'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2663c1-af8e-46bc-8bb5-52643106bdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Sample (Self-Attention in BERT):\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"I need to transfer $500 to my savings account.\"\n",
    "\n",
    "# Tokenize and get attention scores\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "# Attention weights of the last layer\n",
    "attentions = outputs.attentions[-1]\n",
    "print(attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05337e07-c4f6-4c87-b212-991f4563c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Sample (Using Transformer in Hugging Face):\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load pre-trained T5 model and tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Example input text\n",
    "input_text = \"Translate English to French: How are you?\"\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate output (translation)\n",
    "outputs = model.generate(inputs['input_ids'], max_length=40)\n",
    "\n",
    "# Decode output\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f0b421-4100-407a-ac61-579a3b2dcd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Sample (Fine-tuning a Pre-trained Model for Sentiment Analysis):\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, BertForSequenceClassification, BertTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a sentiment analysis dataset (e.g., IMDb)\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    evaluation_strategy=\"epoch\",    \n",
    "    per_device_train_batch_size=8,  \n",
    "    num_train_epochs=3,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=tokenized_datasets['train'],         \n",
    "    eval_dataset=tokenized_datasets['test']\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2219545-cb66-4c1a-bea0-bea78e2ec978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Sample (Contextualized Representations using BERT):\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example sentence with the word \"bank\"\n",
    "sentence = \"I went to the river bank.\"\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# Get contextualized embeddings\n",
    "outputs = model(**inputs)\n",
    "embeddings = outputs.last_hidden_state\n",
    "\n",
    "# Display embeddings for the word \"bank\" (token index 3)\n",
    "print(embeddings[0][3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5abb69-3cd5-46f3-97e2-da3059c37172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Sample (Autoregressive Generation with GPT-2):\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Example prompt for generating text\n",
    "prompt = \"How do I apply for a loan at your bank?\"\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text (autoregressive generation)\n",
    "outputs = model.generate(inputs['input_ids'], max_length=50)\n",
    "\n",
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2152c4-4443-4dd4-bb31-137545421136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Sample (Using GLUE Dataset for Text Classification with BERT):\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the GLUE dataset (SST-2 for sentiment analysis)\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    evaluation_strategy=\"epoch\",    \n",
    "    per_device_train_batch_size=8,  \n",
    "    num_train_epochs=3,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=tokenized_datasets['train'],         \n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")\n",
    "\n",
    "# Fine-tune the model on GLUE dataset\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3db0f7b-0778-46cf-b4be-d4ed64ded646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Sample (Using SuperGLUE Dataset):\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the SuperGLUE dataset (e.g., the \"BoolQ\" task for question answering)\n",
    "dataset = load_dataset(\"super_glue\", \"boolq\")\n",
    "\n",
    "# Load pre-trained RoBERTa model and tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['question'], examples['passage'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    evaluation_strategy=\"epoch\",    \n",
    "    per_device_train_batch_size=8,  \n",
    "    num_train_epochs=3,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=tokenized_datasets['train'],         \n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")\n",
    "\n",
    "# Fine-tune the model on SuperGLUE dataset\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caafe947-1961-42e3-847c-213520add12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Sample (Using SQuAD for Question Answering):\n",
    "\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the SQuAD dataset\n",
    "dataset = load_dataset(\"squad\")\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the dataset for question answering\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['question'], examples['context'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    evaluation_strategy=\"epoch\",    \n",
    "    per_device_train_batch_size=8,  \n",
    "    num_train_epochs=3,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=tokenized_datasets['train'],         \n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")\n",
    "\n",
    "# Fine-tune the model on SQuAD dataset\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbdd6cb-c64d-4a2d-845e-c516c8ee8760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Sample (Zero-Shot Classification using Hugging Face):\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load zero-shot classification pipeline\n",
    "zero_shot_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Example text and candidate labels\n",
    "sequence = \"How do I apply for a personal loan?\"\n",
    "candidate_labels = [\"finance\", \"health\", \"technology\", \"sports\"]\n",
    "\n",
    "# Perform zero-shot classification\n",
    "result = zero_shot_classifier(sequence, candidate_labels)\n",
    "\n",
    "# Display result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb39152-f959-4a22-a5a2-957f7f68156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Sample (Using ROUGE for Text Generation Evaluation):\n",
    "\n",
    "from datasets import load_metric\n",
    "\n",
    "# Load the ROUGE metric\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "# Example of generated and reference texts\n",
    "generated_text = \"To open a savings account, visit our branch.\"\n",
    "reference_text = \"Visit our branch to open a savings account.\"\n",
    "\n",
    "# Evaluate the generated text\n",
    "results = rouge.compute(predictions=[generated_text], references=[reference_text])\n",
    "\n",
    "# Display ROUGE score\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698b1b27-a065-4874-8857-06a9e6a5019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Sample (Using EvalResults for Multiple Metrics):\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Load dataset and model\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and prepare dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c30ca91-8140-4ae7-9cfd-5894c354fc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Sample (Accuracy for Text Classification Task):\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the GLUE dataset (SST-2 task for sentiment analysis)\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model and calculate accuracy\n",
    "predictions, labels, _ = trainer.predict(tokenized_datasets['validation'])\n",
    "predicted_labels = predictions.argmax(axis=-1)\n",
    "accuracy = accuracy_score(labels, predicted_labels)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9ab785-96c3-466a-a44f-7adfccaf202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Sample (Precision, Recall, F1-Score for Text Classification):\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate Precision, Recall, and F1-Score\n",
    "precision = precision_score(labels, predicted_labels)\n",
    "recall = recall_score(labels, predicted_labels)\n",
    "f1 = f1_score(labels, predicted_labels)\n",
    "\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-Score: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f875791-4b97-4655-b15b-2ab0470c937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Sample (Using ROUGE for Text Summarization):\n",
    "\n",
    "from datasets import load_metric\n",
    "\n",
    "# Load ROUGE metric\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "# Example generated and reference texts for summarization\n",
    "generated_text = \"The customer requested a loan to buy a new house.\"\n",
    "reference_text = \"A customer applied for a home loan.\"\n",
    "\n",
    "# Compute ROUGE score\n",
    "results = rouge.compute(predictions=[generated_text], references=[reference_text])\n",
    "\n",
    "# Display ROUGE score\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1347bfe-8a72-453f-9d7f-b975ce9e0521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Sample (Using BLEU for Text Generation Evaluation):\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Example generated and reference sentences for translation or generation\n",
    "generated_text = \"The loan application was processed successfully.\"\n",
    "reference_text = [\"The loan application has been successfully processed.\"]\n",
    "\n",
    "# Compute BLEU score\n",
    "bleu_score = sentence_bleu(reference_text, generated_text.split())\n",
    "\n",
    "# Display BLEU score\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a353a6-0a09-4a78-8a9a-8655b0cbd7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Sample (Using Perplexity for Language Modeling):\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load GPT2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Example text for language modeling\n",
    "input_text = \"The bank offers various services.\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Compute loss and perplexity\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    loss = outputs.loss\n",
    "    perplexity = torch.exp(loss)\n",
    "\n",
    "print(f\"Perplexity: {perplexity.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15110304-1921-4fcf-bd6e-96333d0c886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Sample (Using MAP for Text Retrieval):\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# Example relevance scores for retrieved documents\n",
    "y_true = [1, 0, 1, 0, 1]  # Relevance of documents\n",
    "y_scores = [0.9, 0.1, 0.75, 0.2, 0.85]  # Model scores for documents\n",
    "\n",
    "# Compute MAP\n",
    "map_score = average_precision_score(y_true, y_scores)\n",
    "\n",
    "print(f\"Mean Average Precision (MAP): {map_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f519ac-f7bb-4c41-a095-3856cfe699b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code Sample (Using ROC AUC for Binary Classification):\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Example model predictions and true labels\n",
    "y_true = [0, 1, 0, 1, 0]  # True labels\n",
    "y_probs = [0.1, 0.9, 0.3, 0.7, 0.2]  # Predicted probabilities for class 1\n",
    "\n",
    "# Compute ROC AUC score\n",
    "roc_auc = roc_auc_score(y_true, y_probs)\n",
    "\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19ba740-a7d0-4d9a-981f-e8d569618804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code: Confidence Scoring with BERT\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.nn.functional import softmax\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer for binary classification (e.g., sentiment analysis)\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Input sentence\n",
    "sentence = \"I love the customer service at this bank!\"\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Apply softmax to get probability scores (confidence)\n",
    "probs = softmax(logits, dim=-1)\n",
    "confidence_score = torch.max(probs).item()\n",
    "predicted_class = torch.argmax(probs).item()\n",
    "\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "print(f\"Confidence Score: {confidence_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7b738e-2f2a-403a-8cd6-3589192a6d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code: Upper and Lower Bound for Accuracy\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Simulated true labels and predictions (for example, in a fraud detection model)\n",
    "true_labels = np.array([0, 1, 1, 0, 1, 0, 1, 1, 0, 0])\n",
    "predictions = np.array([0, 1, 0, 0, 1, 0, 1, 1, 1, 0])\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "# Compute lower bound (accuracy of a random classifier)\n",
    "random_accuracy = np.mean(np.random.choice([0, 1], size=true_labels.shape[0]))\n",
    "\n",
    "# Compute upper bound (accuracy of a perfect classifier)\n",
    "perfect_accuracy = 1.0  # Ideal accuracy for perfect predictions\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Random Classifier Accuracy (Lower Bound): {random_accuracy:.4f}\")\n",
    "print(f\"Perfect Classifier Accuracy (Upper Bound): {perfect_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246d99f-849c-41c3-8a7d-53af67d71beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code: Hyperparameter Optimization using GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load a sample dataset (Iris)\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define parameter grid (upper and lower bounds for hyperparameters)\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],  # Regularization strength (lower and upper bounds)\n",
    "    'kernel': ['linear', 'rbf'],  # Type of SVM kernel\n",
    "    'gamma': [0.001, 0.01, 0.1]  # Kernel coefficient (lower and upper bounds)\n",
    "}\n",
    "\n",
    "# Initialize the model and perform GridSearchCV\n",
    "svc = SVC()\n",
    "grid_search = GridSearchCV(svc, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output best parameters and model performance\n",
    "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Accuracy: {grid_search.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cba8cc-325e-4d39-8bc5-92c691894c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python Code: Overfitting in Logistic Regression\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression model with a high degree of complexity (C=100)\n",
    "model = LogisticRegression(C=100)  # Higher C value leads to more complex model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on training and test data\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy on training and test data\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453dd998-9f19-402f-9dd4-beba15c36efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Code: Regularization in Logistic Regression (L2 Regularization)\n",
    "\n",
    "# Train a logistic regression model with L2 regularization (Ridge)\n",
    "model_ridge = LogisticRegression(C=1)  # Lower C value means stronger regularization\n",
    "model_ridge.fit(X_train, y_train)\n",
    "\n",
    "# Predict on training and test data\n",
    "y_train_pred_ridge = model_ridge.predict(X_train)\n",
    "y_test_pred_ridge = model_ridge.predict(X_test)\n",
    "\n",
    "# Calculate accuracy on training and test data\n",
    "train_accuracy_ridge = accuracy_score(y_train, y_train_pred_ridge)\n",
    "test_accuracy_ridge = accuracy_score(y_test, y_test_pred_ridge)\n",
    "\n",
    "print(f\"Training Accuracy with Regularization: {train_accuracy_ridge:.4f}\")\n",
    "print(f\"Testing Accuracy with Regularization: {test_accuracy_ridge:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c47c1a7-0e75-469f-a5bb-bc8d7ac825d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Code: Cross-Validation to Detect Overfitting\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Cross-validation with a logistic regression model\n",
    "model_cv = LogisticRegression(C=1)  # Regularized model to avoid overfitting\n",
    "cv_scores = cross_val_score(model_cv, X, y, cv=5)  # 5-fold cross-validation\n",
    "\n",
    "print(f\"Cross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean cross-validation score: {np.mean(cv_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8520b60-1f8e-4f6a-bbc0-4ce42ae21f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Code: Simulating a Broad Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a synthetic broad dataset for loan approval prediction\n",
    "n_samples = 1000\n",
    "n_features = 15  # Number of features in the dataset\n",
    "\n",
    "# Generating random features (e.g., age, income, credit score, etc.)\n",
    "data = {\n",
    "    'age': np.random.randint(18, 70, size=n_samples),\n",
    "    'income': np.random.randint(20000, 120000, size=n_samples),\n",
    "    'credit_score': np.random.randint(300, 850, size=n_samples),\n",
    "    'loan_amount': np.random.randint(5000, 50000, size=n_samples),\n",
    "    'debt_to_income_ratio': np.random.uniform(0, 1, size=n_samples),\n",
    "    'employment_length': np.random.randint(1, 30, size=n_samples),\n",
    "    'monthly_expenses': np.random.randint(1000, 5000, size=n_samples),\n",
    "    'savings_balance': np.random.randint(0, 20000, size=n_samples),\n",
    "    'is_homeowner': np.random.choice([0, 1], size=n_samples),\n",
    "    'education_level': np.random.choice(['High School', 'Undergraduate', 'Graduate'], size=n_samples),\n",
    "    'loan_history': np.random.choice([0, 1], size=n_samples),  # 0 = No previous loan, 1 = Previous loan taken\n",
    "    'marital_status': np.random.choice(['Single', 'Married', 'Divorced'], size=n_samples),\n",
    "    'num_dependents': np.random.randint(0, 5, size=n_samples),\n",
    "    'region': np.random.choice(['Urban', 'Suburban', 'Rural'], size=n_samples),\n",
    "    'employment_status': np.random.choice(['Employed', 'Self-Employed', 'Unemployed'], size=n_samples)\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Target variable: Whether the loan was approved (1 = Approved, 0 = Rejected)\n",
    "df['loan_approval'] = np.random.choice([0, 1], size=n_samples)\n",
    "\n",
    "# Show the first few rows of the dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229ab094-6e67-4623-a60f-2861e3bad737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Code: Dimensionality Reduction Using PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Select numerical columns for PCA (excluding the target column 'loan_approval')\n",
    "numerical_columns = ['age', 'income', 'credit_score', 'loan_amount', 'debt_to_income_ratio',\n",
    "                     'employment_length', 'monthly_expenses', 'savings_balance', 'num_dependents']\n",
    "X = df[numerical_columns]\n",
    "\n",
    "# Standardize the data (important for PCA)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA to reduce the dimensionality\n",
    "pca = PCA(n_components=5)  # Reducing to 5 components\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Include the target variable\n",
    "y = df['loan_approval']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a model on the reduced dataset (Random Forest for classification)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy on test set after PCA: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9b8a42-7c8a-4e2d-84bf-6eb77884f521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Code: Feature Selection with Lasso (L1 Regularization)\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Use the numerical columns for training\n",
    "X = df[numerical_columns]\n",
    "y = df['loan_approval']\n",
    "\n",
    "# Standardize the data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply Lasso Regression for feature selection\n",
    "lasso = LassoCV(cv=5)\n",
    "lasso.fit(X_scaled, y)\n",
    "\n",
    "# Print the coefficients to see which features were selected\n",
    "selected_features = np.array(numerical_columns)[lasso.coef_ != 0]\n",
    "print(f\"Selected features by Lasso: {selected_features}\")\n",
    "\n",
    "# Train a Random Forest Classifier using only selected features\n",
    "X_selected = X[selected_features]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy after feature selection: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a05172c-a78e-48b5-bc10-e4e1616e9862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Code: Train-Test Split and Model Evaluation for Loan Approval\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Generate a synthetic dataset for loan approval prediction\n",
    "np.random.seed(42)\n",
    "\n",
    "# Creating a synthetic dataset with 1000 samples\n",
    "data = {\n",
    "    'age': np.random.randint(18, 70, size=1000),\n",
    "    'income': np.random.randint(20000, 120000, size=1000),\n",
    "    'credit_score': np.random.randint(300, 850, size=1000),\n",
    "    'loan_amount': np.random.randint(5000, 50000, size=1000),\n",
    "    'debt_to_income_ratio': np.random.uniform(0, 1, size=1000),\n",
    "    'employment_length': np.random.randint(1, 30, size=1000),\n",
    "    'loan_approval': np.random.choice([0, 1], size=1000)  # Target variable: 0 = Rejected, 1 = Approved\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Features and target variable\n",
    "X = df.drop(columns=['loan_approval'])\n",
    "y = df['loan_approval']\n",
    "\n",
    "# Split the data into training and testing sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the loan approval on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d63a3f-0bc1-4a14-b6ec-45af7c10a6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Code: Cross-Validation Using K-Fold for Loan Approval\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Use the entire dataset\n",
    "X = df.drop(columns=['loan_approval'])\n",
    "y = df['loan_approval']\n",
    "\n",
    "# Initialize a RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "print(f\"Cross-Validation Accuracy Scores: {cv_scores}\")\n",
    "print(f\"Mean Accuracy: {cv_scores.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a03ba4-b722-4282-a03b-848c062f6654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Code: Simulating Fraud Detection and Finite Assessment\n",
    "\n",
    "# Generate synthetic fraud detection dataset\n",
    "data_fraud = {\n",
    "    'transaction_amount': np.random.randint(5, 1000, size=1000),\n",
    "    'transaction_frequency': np.random.randint(1, 20, size=1000),\n",
    "    'is_international': np.random.choice([0, 1], size=1000),\n",
    "    'is_fraud': np.random.choice([0, 1], size=1000)  # 0 = No Fraud, 1 = Fraud\n",
    "}\n",
    "\n",
    "df_fraud = pd.DataFrame(data_fraud)\n",
    "\n",
    "# Features and target variable\n",
    "X_fraud = df_fraud.drop(columns=['is_fraud'])\n",
    "y_fraud = df_fraud['is_fraud']\n",
    "\n",
    "# Train-test split\n",
    "X_train_fraud, X_test_fraud, y_train_fraud, y_test_fraud = train_test_split(X_fraud, y_fraud, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a RandomForest Classifier for fraud detection\n",
    "model_fraud = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_fraud.fit(X_train_fraud, y_train_fraud)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred_fraud = model_fraud.predict(X_test_fraud)\n",
    "accuracy_fraud = accuracy_score(y_test_fraud, y_pred_fraud)\n",
    "\n",
    "print(f\"Fraud Detection Model Accuracy: {accuracy_fraud:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e0e582-5ad6-4e44-b8c3-a60bc65c3962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer Support Automation (Text Classification): In a banking scenario, LLMs can be used to classify customer queries into \n",
    "# different categories like \"Loan Inquiry\", \"Account Balance\", \"Transaction Issue\", etc.\n",
    "# Example Code using transformers library:\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "classifier = pipeline(\"zero-shot-classification\", model=model_name)\n",
    "\n",
    "# Sample banking customer query\n",
    "query = \"I want to check my loan balance\"\n",
    "\n",
    "# Define candidate labels\n",
    "candidate_labels = [\"Loan Inquiry\", \"Account Balance\", \"Transaction Issue\", \"Customer Support\"]\n",
    "\n",
    "# Classify the query\n",
    "result = classifier(query, candidate_labels)\n",
    "print(result)\n",
    "This model will classify the query as \"Loan Inquiry\", based on the available labels. This can be useful in automating customer service.\n",
    "2. Fraud Detection (Sentiment Analysis)\n",
    "Sentiment analysis can be leveraged to identify potential fraudulent activity by detecting abnormal or suspicious patterns in customer communications.\n",
    "Example Code for Sentiment Analysis:\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained sentiment-analysis model\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Example banking transaction message\n",
    "transaction_message = \"I did not make this transaction, please help!\"\n",
    "\n",
    "# Analyze sentiment\n",
    "result = sentiment_analyzer(transaction_message)\n",
    "print(result)\n",
    "The result might show negative sentiment, which could indicate a potential fraudulent activity or an issue the customer needs to resolve.\n",
    "3. Loan Risk Assessment (Text Summarization)\n",
    "Loan applications often come with large amounts of documentation. Using LLMs for summarizing documents can significantly improve decision-making processes for loan risk assessments.\n",
    "Example Code for Document Summarization:\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained summarization model\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "# Sample loan application text\n",
    "loan_application_text = \"\"\"\n",
    "John Doe has been employed for over 5 years at Tech Corp, earning a monthly salary of $5000. His credit score is 720.\n",
    "He is requesting a loan of $20,000 to purchase a car. His monthly expenses include rent of $1500 and student loan payments\n",
    "of $200. He has no previous history of loan defaults.\n",
    "\"\"\"\n",
    "\n",
    "# Generate summary\n",
    "summary = summarizer(loan_application_text, max_length=100, min_length=50, do_sample=False)\n",
    "print(summary)\n",
    "This would generate a concise summary of the loan application, highlighting the key points that will help assess the loan's risk.\n",
    "4. Automated Report Generation (Text Generation)\n",
    "LLMs can automatically generate reports, such as quarterly financial summaries or audit reports, using natural language generation.\n",
    "Example Code for Text Generation:\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained text-generation model\n",
    "generator = pipeline(\"text-generation\", model=\"gpt-2\")\n",
    "\n",
    "# Input prompt for generating a financial report\n",
    "prompt = \"Generate a summary of the bank's financial performance for Q4 2024.\"\n",
    "\n",
    "# Generate the report\n",
    "report = generator(prompt, max_length=150)\n",
    "print(report[0]['generated_text'])\n",
    "This model will generate a summary or report based on the provided prompt. For the banking scenario, this can be used to automate report generation for monthly or quarterly financial summaries.\n",
    "5. Multi-lingual Customer Support (Machine Translation)\n",
    "For banks with international customers, an LLM can be used for language translation, enabling support for multiple languages.\n",
    "Example Code for Machine Translation:\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained translation model (from English to French)\n",
    "translator = pipeline(\"translation_en_to_fr\", model=\"t5-small\")\n",
    "\n",
    "# Sample customer query in English\n",
    "query = \"What is the status of my loan application?\"\n",
    "\n",
    "# Translate the query to French\n",
    "translation = translator(query)\n",
    "print(translation)\n",
    "# This model would translate the customer query into French, allowing customer support agents or automated systems \n",
    "     # to respond in the customer's preferred language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af37d84e-3b84-46fb-a97b-4db1983a25c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Example for Document Verification (OCR + Text Processing)\n",
    "\n",
    "from transformers import pipeline\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "# OCR: Extract text from image (for document verification)\n",
    "image_path = \"bank_statement.jpg\"\n",
    "img = Image.open(image_path)\n",
    "text_from_image = pytesseract.image_to_string(img)\n",
    "\n",
    "# Use a pre-trained model for text classification (e.g., checking if the extracted text is related to banking)\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"distilbert-base-uncased\")\n",
    "\n",
    "candidate_labels = [\"Loan Inquiry\", \"Account Balance\", \"Transaction Issue\", \"Bank Statement\"]\n",
    "result = classifier(text_from_image, candidate_labels)\n",
    "print(f\"Extracted Text: {text_from_image}\")\n",
    "print(f\"Classification Result: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a682aa-b112-4db3-8cd4-2809d8195589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Example for Text + Audio (Speech-to-Text + Sentiment Analysis)\n",
    "\n",
    "from transformers import pipeline\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Initialize speech recognizer\n",
    "recognizer = sr.Recognizer()\n",
    "audio_path = \"customer_call.wav\"\n",
    "\n",
    "# Convert speech to text\n",
    "with sr.AudioFile(audio_path) as source:\n",
    "    audio_data = recognizer.record(source)\n",
    "    transcript = recognizer.recognize_google(audio_data)\n",
    "\n",
    "# Sentiment Analysis on the transcribed text\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "sentiment = sentiment_analyzer(transcript)\n",
    "\n",
    "print(f\"Transcript: {transcript}\")\n",
    "print(f\"Sentiment: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e3c07e-5fc6-41b2-8382-2edea6a96cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Example for Text + Video (Video Summarization)\n",
    "\n",
    "from transformers import pipeline\n",
    "import moviepy.editor as mp\n",
    "\n",
    "# Load the video file\n",
    "video_path = \"bank_video.mp4\"\n",
    "video = mp.VideoFileClip(video_path)\n",
    "\n",
    "# Extract audio from video for further processing\n",
    "audio = video.audio\n",
    "audio.write_audiofile(\"extracted_audio.wav\")\n",
    "\n",
    "# You could use pre-trained video captioning models, but here we'll use basic summarization for text\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "# Example text from a video (e.g., transcript of a training session)\n",
    "transcript = \"\"\"\n",
    "In this video, we will go through the basic features of our mobile banking app, including how to check your balance,\n",
    "transfer funds, and review recent transactions.\n",
    "\"\"\"\n",
    "\n",
    "# Generate summary\n",
    "summary = summarizer(transcript, max_length=100, min_length=50, do_sample=False)\n",
    "print(\"Video Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffacfbb0-5555-49e1-81c5-b30722708d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Example for Text + Image + Audio (Fraud Detection)\n",
    "\n",
    "from transformers import pipeline\n",
    "import pytesseract\n",
    "import speech_recognition as sr\n",
    "from PIL import Image\n",
    "\n",
    "# Step 1: Extract text from image (OCR for scanned document)\n",
    "image_path = \"scanned_fraud_report.jpg\"\n",
    "img = Image.open(image_path)\n",
    "text_from_image = pytesseract.image_to_string(img)\n",
    "\n",
    "# Step 2: Transcribe audio (Customer call)\n",
    "recognizer = sr.Recognizer()\n",
    "audio_path = \"customer_call.wav\"\n",
    "with sr.AudioFile(audio_path) as source:\n",
    "    audio_data = recognizer.record(source)\n",
    "    transcript = recognizer.recognize_google(audio_data)\n",
    "\n",
    "# Step 3: Analyze the text from both sources (OCR and transcript)\n",
    "text = text_from_image + \" \" + transcript\n",
    "\n",
    "# Sentiment Analysis for detecting potential fraud\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "sentiment = sentiment_analyzer(text)\n",
    "\n",
    "# Final decision based on sentiment analysis\n",
    "if sentiment[0]['label'] == 'NEGATIVE':\n",
    "    print(\"Fraud Detected: Review required\")\n",
    "else:\n",
    "    print(\"No fraud detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cb600e-69b1-433e-b928-62f4b5599b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multimodal Fraud Detection (Text + Image + Audio)\n",
    "# Scenario: A bank wants to detect potential fraud by analyzing customer communication (audio), transaction data (text), \n",
    "# and submitted documents (images).\n",
    "# Python Code for Fraud Detection\n",
    "\n",
    "from transformers import pipeline\n",
    "import pytesseract\n",
    "import speech_recognition as sr\n",
    "from PIL import Image\n",
    "\n",
    "# Step 1: OCR for Document Verification (Image modality)\n",
    "image_path = \"customer_id_card.jpg\"\n",
    "img = Image.open(image_path)\n",
    "document_text = pytesseract.image_to_string(img)\n",
    "\n",
    "# Step 2: Speech-to-Text for Customer Call (Audio modality)\n",
    "recognizer = sr.Recognizer()\n",
    "audio_path = \"customer_call.wav\"\n",
    "with sr.AudioFile(audio_path) as source:\n",
    "    audio_data = recognizer.record(source)\n",
    "    transcript = recognizer.recognize_google(audio_data)\n",
    "\n",
    "# Step 3: Combine text from both sources (OCR and Transcript)\n",
    "combined_text = document_text + \" \" + transcript\n",
    "\n",
    "# Step 4: Perform Sentiment Analysis to check for Fraudulent Behavior (Text modality)\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "sentiment = sentiment_analyzer(combined_text)\n",
    "\n",
    "# Detect fraud based on sentiment\n",
    "if sentiment[0]['label'] == 'NEGATIVE':\n",
    "    print(\"Fraud Detected: Review Required\")\n",
    "else:\n",
    "    print(\"No fraud detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1b106d-7e7b-45cc-aad6-9ad52b77ab8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multimodal Customer Support Automation (Text + Audio)\n",
    "# Scenario: A bank wants to automate customer support by analyzing text queries (e.g., account balance inquiries) \n",
    "# and audio data (e.g., spoken complaints).\n",
    "# Python Code for Customer Support Automation\n",
    "\n",
    "from transformers import pipeline\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Initialize speech recognizer for audio (customer query)\n",
    "recognizer = sr.Recognizer()\n",
    "audio_path = \"customer_query.wav\"\n",
    "\n",
    "# Convert speech to text\n",
    "with sr.AudioFile(audio_path) as source:\n",
    "    audio_data = recognizer.record(source)\n",
    "    query_text = recognizer.recognize_google(audio_data)\n",
    "\n",
    "# Use a pre-trained text classifier to understand customer intent\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"distilbert-base-uncased\")\n",
    "\n",
    "# Candidate labels for classification\n",
    "candidate_labels = [\"Account Balance Inquiry\", \"Loan Inquiry\", \"Complaint\", \"Other\"]\n",
    "\n",
    "# Classify customer query intent\n",
    "result = classifier(query_text, candidate_labels)\n",
    "\n",
    "print(f\"Customer Query: {query_text}\")\n",
    "print(f\"Query Classification: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64af131-3f16-4b96-b142-676519b3cff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multimodal Document Verification (Text + Image)\n",
    "# Scenario: A bank needs to verify a customers identity by analyzing both the text in a scanned document and the \n",
    "# image content (e.g., facial recognition from a photo ID).\n",
    "# Python Code for Document Verification\n",
    "\n",
    "from transformers import pipeline\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "# Step 1: Extract text from the document image (OCR)\n",
    "document_image_path = \"customer_document.jpg\"\n",
    "doc_img = Image.open(document_image_path)\n",
    "document_text = pytesseract.image_to_string(doc_img)\n",
    "\n",
    "# Step 2: Use a pre-trained text classifier to verify document information\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"distilbert-base-uncased\")\n",
    "candidate_labels = [\"Identity Verification\", \"Account Information\", \"Transaction Details\"]\n",
    "document_classification = classifier(document_text, candidate_labels)\n",
    "\n",
    "print(f\"Document Text: {document_text}\")\n",
    "print(f\"Document Classification: {document_classification}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff0c0b8-5337-48e1-950d-a9bdbf0e97cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing (Loading and Preparing Banking Data)\n",
    "# One of the first steps when working with machine learning in banking is data preprocessing. This can involve loading data, \n",
    "# cleaning, normalizing, and transforming it into a format suitable for training models.\n",
    "# Banking Scenario: Customer Loan Data Preprocessing\n",
    "# We might have a dataset of customers with information like age, income, credit score, loan amount, and other features. \n",
    "# These need to be preprocessed before using them in a machine learning model.\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'age': [25, 30, 45, 35, 50],\n",
    "    'income': [50000, 60000, 100000, 70000, 120000],\n",
    "    'credit_score': [700, 650, 800, 720, 850],\n",
    "    'loan_amount': [20000, 25000, 50000, 30000, 60000],\n",
    "    'approved': [1, 0, 1, 1, 0]  # 1 = Approved, 0 = Denied\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Features and target variable\n",
    "X = df.drop('approved', axis=1)\n",
    "y = df['approved']\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train)\n",
    "\n",
    "# 2. Building a Neural Network for Loan Approval Prediction\n",
    "# In banking, predicting whether a customer will be approved for a loan is a common use case. A neural network can be \n",
    "# built using TensorFlow to classify whether a loan should be approved or denied based on features such as age, income, and credit score.\n",
    "# Banking Scenario: Loan Approval Prediction using Neural Network\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential([\n",
    "    Dense(32, input_dim=X_train.shape[1], activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Output layer with sigmoid for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=10)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# 3. Predicting Credit Card Fraud Detection\n",
    "# Fraud detection is a major application of machine learning in banking. A model can be trained to predict fraudulent credit card transactions.\n",
    "# Banking Scenario: Credit Card Fraud Detection\n",
    "# We can build a neural network model to predict whether a credit card transaction is fraudulent based on features \n",
    "# such as transaction amount, merchant, and time.\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Simulated fraud detection data\n",
    "data = {\n",
    "    'amount': [100, 200, 300, 400, 500],\n",
    "    'merchant': [1, 2, 1, 3, 2],\n",
    "    'time': [10, 15, 20, 25, 30],\n",
    "    'is_fraud': [0, 1, 0, 1, 0]  # 1 = Fraud, 0 = No fraud\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Features and target variable\n",
    "X = df.drop('is_fraud', axis=1)\n",
    "y = df['is_fraud']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build neural network model for fraud detection\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=X_train.shape[1], activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Output layer with sigmoid for binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=10)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# 4. Predicting Customer Churn\n",
    "# Churn prediction is an essential task in the banking industry to retain customers. A churn prediction model predicts the \n",
    "# likelihood of a customer leaving the bank based on factors like transaction history, customer support interactions, etc.\n",
    "# Banking Scenario: Customer Churn Prediction\n",
    "# In this case, we can predict whether a customer will churn based on their behavior and interactions with the bank.\n",
    "\n",
    "# Simulated churn data\n",
    "data = {\n",
    "    'age': [25, 30, 45, 35, 50],\n",
    "    'transaction_frequency': [5, 2, 8, 3, 1],\n",
    "    'balance': [1000, 1500, 3000, 1200, 400],\n",
    "    'churned': [0, 1, 0, 0, 1]  # 1 = Churned, 0 = Not Churned\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Features and target variable\n",
    "X = df.drop('churned', axis=1)\n",
    "y = df['churned']\n",
    "\n",
    "# Normalize the features\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build churn prediction model\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=X_train.shape[1], activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Output layer with sigmoid for binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=10)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# 5. Forecasting Banking Stock Prices (Time Series Analysis)\n",
    "# Banks may also use machine learning to predict future stock prices. A simple time series forecasting model can be \n",
    "# created using LSTM (Long Short-Term Memory), a type of recurrent neural network (RNN) suitable for sequential data.\n",
    "# Banking Scenario: Stock Price Prediction using LSTM\n",
    "# Here, we will use LSTM to predict future stock prices based on historical data.\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "# Simulate historical stock price data\n",
    "stock_data = [100, 101, 102, 103, 105, 106, 107, 108, 109, 110]  # Stock prices\n",
    "\n",
    "# Prepare the data\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step):\n",
    "        X.append(data[i:i + time_step])\n",
    "        y.append(data[i + time_step])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "time_step = 3\n",
    "X, y = create_dataset(stock_data, time_step)\n",
    "\n",
    "# Reshaping for LSTM [samples, time steps, features]\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(units=50, return_sequences=True, input_shape=(X.shape[1], 1)),\n",
    "    LSTM(units=50),\n",
    "    Dense(1)  # Output layer for regression (continuous value)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=100, batch_size=1, verbose=2)\n",
    "\n",
    "# Predict the next stock price\n",
    "predicted_price = model.predict(X[-1].reshape(1, time_step, 1))\n",
    "print(f\"Predicted Next Stock Price: {predicted_price[0][0]}\")\n",
    "\n",
    "# 6. Loan Default Prediction with TensorFlow Decision Forests (TF-DF)\n",
    "# TensorFlow Decision Forests (TF-DF) is an alternative to deep learning models, focusing on decision trees \n",
    "# and ensemble methods, suitable for financial applications like loan default prediction.\n",
    "\n",
    "import tensorflow_decision_forests as tfdf\n",
    "\n",
    "# Load data (for the sake of example, assume df is preprocessed)\n",
    "data = {\n",
    "    'age': [25, 30, 45, 35, 50],\n",
    "    'income': [50000, 60000, 100000, 70000, 120000],\n",
    "    'loan_amount': [20000, 25000, 50000, 30000, 60000],\n",
    "    'defaulted': [1, 0, 0, 0, 1]  # 1 = Defaulted, 0 = No Default\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert data to TensorFlow Decision Forests dataset\n",
    "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(df, task=tfdf.keras.Task.CLASSIFICATION, label=\"defaulted\")\n",
    "\n",
    "# Train a Random Forest model\n",
    "model = tfdf.keras.RandomForestModel(task=tfdf.keras.Task.CLASSIFICATION)\n",
    "model.fit(train_ds)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(train_ds)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73f131d-c8fe-4439-93bb-4f62ab4f7718",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  1: Loan Approval Prediction Using Softmax\n",
    "# In this scenario, we have a model predicting the loan approval status for a customer based on features like income, \n",
    "# credit score, etc. Instead of a binary classification (approved/denied), we predict multiple possible outcomes like:\n",
    "# 1.\tLoan Approved\n",
    "# 2.\tLoan Denied\n",
    "# 3.\tPending Review\n",
    "# Step-by-Step Code:\n",
    "# 1.\tSimulate Customer Data\n",
    "# 2.\tTrain a Model\n",
    "# 3.\tApply Softmax to Predict Probabilities\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample banking data: [income, credit_score, loan_amount]\n",
    "data = {\n",
    "    'income': [50000, 60000, 100000, 70000, 120000],\n",
    "    'credit_score': [700, 650, 800, 720, 850],\n",
    "    'loan_amount': [20000, 25000, 50000, 30000, 60000],\n",
    "    'approval_status': [0, 1, 0, 1, 2]  # 0 = Approved, 1 = Denied, 2 = Pending\n",
    "}\n",
    "\n",
    "# Convert data to DataFrame\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Features and target variable\n",
    "X = df.drop('approval_status', axis=1)\n",
    "y = df['approval_status']\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model for multi-class classification\n",
    "model = Sequential([\n",
    "    Dense(32, input_dim=X_train.shape[1], activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # 3 classes: Approved, Denied, Pending\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=10)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Predict probabilities with softmax\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Display predicted probabilities for each class (Approved, Denied, Pending)\n",
    "print(\"Predicted Probabilities:\")\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d377dbe-44c2-4523-8391-6438e2fa4a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 2: Predicting Customer Churn Using Softmax\n",
    "# In this scenario, we predict the likelihood of a customer churning (leaving the bank) based on \n",
    "# customer behavior. The possible outcomes can be:\n",
    "# 1.\tWill Stay\n",
    "# 2.\tWill Churn\n",
    "# 3.\tUncertain\n",
    "# This problem can be modeled as a multi-class classification problem with Softmax.\n",
    "\n",
    "# Simulated churn data\n",
    "churn_data = {\n",
    "    'age': [25, 30, 45, 35, 50],\n",
    "    'transaction_frequency': [5, 2, 8, 3, 1],\n",
    "    'balance': [1000, 1500, 3000, 1200, 400],\n",
    "    'churn_status': [0, 1, 0, 1, 2]  # 0 = Will Stay, 1 = Will Churn, 2 = Uncertain\n",
    "}\n",
    "\n",
    "df_churn = pd.DataFrame(churn_data)\n",
    "\n",
    "# Features and target variable\n",
    "X_churn = df_churn.drop('churn_status', axis=1)\n",
    "y_churn = df_churn['churn_status']\n",
    "\n",
    "# Normalize the features\n",
    "X_scaled_churn = scaler.fit_transform(X_churn)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train_churn, X_test_churn, y_train_churn, y_test_churn = train_test_split(X_scaled_churn, y_churn, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the neural network model for customer churn prediction\n",
    "model_churn = Sequential([\n",
    "    Dense(64, input_dim=X_train_churn.shape[1], activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # 3 classes: Will Stay, Will Churn, Uncertain\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_churn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model_churn.fit(X_train_churn, y_train_churn, epochs=100, batch_size=10)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model_churn.evaluate(X_test_churn, y_test_churn)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Predict probabilities with softmax\n",
    "churn_predictions = model_churn.predict(X_test_churn)\n",
    "\n",
    "# Display predicted probabilities for each class (Will Stay, Will Churn, Uncertain)\n",
    "print(\"Predicted Probabilities for Customer Churn:\")\n",
    "print(churn_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d6e9f9-91b5-4e3f-8b54-1abadcc9a212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1: Check Processing and Signature Verification\n",
    "# Banks receive checks for deposit, and it's essential to automate the process of recognizing check images, \n",
    "# reading the details (e.g., account number, check amount), and verifying the signature.\n",
    "# Application of Image Recognition:\n",
    "# \tOptical Character Recognition (OCR) to read check details such as account number, check number, and amount.\n",
    "# \tSignature verification to ensure that the signature on the check matches the one on file.\n",
    "# Step-by-Step Code for Check Processing and OCR:\n",
    "# We'll use the Tesseract OCR engine, which is an open-source OCR tool, along with Python libraries like pytesseract and OpenCV.\n",
    "# Installing Required Libraries:\n",
    "\n",
    "!pip install pytesseract opencv-python\n",
    "# Code for Extracting Text from Check Images (OCR):\n",
    "\n",
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "# Path to Tesseract executable (update if required on your system)\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "# Read the check image\n",
    "image_path = 'check_image.jpg'  # Sample check image file\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# Convert the image to grayscale for better OCR accuracy\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply thresholding to get a binary image (enhance text visibility)\n",
    "_, binary_image = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Use pytesseract to extract text from the image\n",
    "extracted_text = pytesseract.image_to_string(binary_image)\n",
    "\n",
    "print(\"Extracted Text from Check:\")\n",
    "print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1472b8c7-2173-4a67-b793-71779401c2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 2: Face Recognition for Identity Verification\n",
    "# Face recognition technology is widely used in banking apps to verify the identity of customers. For example, it can be \n",
    "# used to allow customers to log in to mobile banking apps or verify the person performing a transaction.\n",
    "# Application of Image Recognition:\n",
    "# \tFace recognition to authenticate customers based on their facial features.\n",
    "# We will use a pre-trained model (like OpenCV's Haar Cascades or a deep learning model like a Convolutional Neural Network \n",
    "# (CNN)) for detecting faces in images.\n",
    "# Step-by-Step Code for Face Detection with OpenCV:\n",
    "# Installing Required Libraries:\n",
    "\n",
    "!pip install opencv-python\n",
    "# Code for Face Recognition:\n",
    "\n",
    "import cv2\n",
    "\n",
    "# Load the pre-trained Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Read the image with the customer's photo\n",
    "image_path = 'customer_photo.jpg'  # Customer's photo\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect faces in the image\n",
    "faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "# Draw a rectangle around each detected face\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "# Display the image with the detected face(s)\n",
    "cv2.imshow('Detected Faces', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a3c91c-44db-445d-88a4-c51546ca8e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  3: Document Classification (Loan Application Forms)\n",
    "# Banks often receive documents like loan application forms, and automating the classification of these documents can \n",
    "# speed up processing. Image recognition can help classify documents into categories like \"Loan Application,\" \"ID Proof,\" \"Address Proof,\" etc.\n",
    "# Application of Image Recognition:\n",
    "# \tDocument classification based on visual features in the form.\n",
    "# In this case, we can use a Convolutional Neural Network (CNN) to classify the images of documents.\n",
    "# Step-by-Step Code for Document Classification with CNN:\n",
    "# Install TensorFlow:\n",
    "\n",
    "!pip install tensorflow\n",
    "# Code for Training a CNN to Classify Documents:\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Set up directories for training and validation images\n",
    "train_dir = 'path_to_train_images'  # Training images folder\n",
    "validation_dir = 'path_to_validation_images'  # Validation images folder\n",
    "\n",
    "# Image preprocessing\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(150, 150), batch_size=32, class_mode='binary')\n",
    "validation_generator = validation_datagen.flow_from_directory(validation_dir, target_size=(150, 150), batch_size=32, class_mode='binary')\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')  # Binary classification (Loan App vs ID Proof)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_generator, epochs=10, validation_data=validation_generator)\n",
    "\n",
    "# Save the model\n",
    "model.save('document_classifier.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe3c45e-8eab-4888-9ef8-c3ffdc06e056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-by-Step # Python Code Example:\n",
    "# 1.\tInstall Required Libraries:\n",
    "# Youll need TensorFlow and Keras to build the CNN, and Matplotlib to visualize the results.\n",
    "\n",
    "!pip install tensorflow matplotlib\n",
    "# 2.\tDataset Preparation:\n",
    "# For this example, lets assume we have a dataset of ATM receipt images. The images are in different folders: one folder \n",
    "# for fraudulent receipts and one for legitimate ones. We will use ImageDataGenerator to load the data and preprocess it.\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define paths to your dataset (e.g., 'data/train' and 'data/validation')\n",
    "train_dir = 'data/train'  # Folder with images of ATM receipts (fraudulent or legitimate)\n",
    "validation_dir = 'data/validation'  # Folder with validation images\n",
    "\n",
    "# Use ImageDataGenerator to rescale images and apply augmentation for better model robustness\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "validation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load images and their labels\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),  # Resize images to 150x150 for consistency\n",
    "    batch_size=32,\n",
    "    class_mode='binary'  # Binary classification: Fraudulent or Legitimate\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "# In this step:\n",
    "# \tImageDataGenerator is used to rescale the images and apply random transformations (like zoom or shear) to enhance the model's robustness.\n",
    "# \tThe data is loaded from directories where each folder contains images of a particular class (fraudulent or legitimate).\n",
    "\n",
    "# 3.\tBuild the CNN Model with Flattening:\n",
    "# Now, lets build a CNN model. After feature extraction through convolutional and pooling layers, we will flatten the \n",
    "# output before feeding it into the fully connected layers.\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential([\n",
    "    # First convolutional layer\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Second convolutional layer\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Third convolutional layer\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Flatten the output from convolutional layers to a 1D vector\n",
    "    layers.Flatten(),  # Flattening layer\n",
    "    \n",
    "    # Fully connected layers\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')  # Binary output: Fraud or Legitimate\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4da42d-ab2f-4766-bda9-b84eb6b65400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can train the model using the data generators created earlier.\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,  # Number of batches per epoch\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50  # Number of validation batches\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('atm_receipt_fraud_detection_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e780417a-027f-45a3-b3d3-22107c5bdbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Model Training:\n",
    "# You can visualize the training and validation accuracy over epochs to see how well the model is performing.\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39015f92-09c7-4126-970c-1a21f7a60ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Code for Tabular Q-Learning:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define the environment (simple banking scenario)\n",
    "states = [0, 1, 2]  # 0: Low balance, 1: Medium balance, 2: High balance\n",
    "actions = [0, 1, 2]  # 0: No action, 1: Offer loan, 2: Send notification\n",
    "\n",
    "# Initialize Q-table with zeros\n",
    "Q = np.zeros((len(states), len(actions)))\n",
    "\n",
    "# Define parameters\n",
    "alpha = 0.1  # learning rate\n",
    "gamma = 0.9  # discount factor\n",
    "episodes = 1000\n",
    "\n",
    "# Rewards for each action-state pair\n",
    "rewards = {\n",
    "    (0, 0): -1, (0, 1): -10, (0, 2): 2,  # Low balance\n",
    "    (1, 0): 0, (1, 1): 2, (1, 2): 1,   # Medium balance\n",
    "    (2, 0): 1, (2, 1): 5, (2, 2): 3   # High balance\n",
    "}\n",
    "\n",
    "# Simulate the learning process\n",
    "for episode in range(episodes):\n",
    "    state = np.random.choice(states)  # Random initial state\n",
    "    \n",
    "    while state != 2:  # Stop if we reach the 'high balance' state\n",
    "        action = np.argmax(Q[state])  # Choose action with max Q-value (greedy policy)\n",
    "        \n",
    "        next_state = np.random.choice(states)  # Transition to a new state (simulated)\n",
    "        \n",
    "        reward = rewards[(state, action)]  # Get the reward for the state-action pair\n",
    "        \n",
    "        # Update the Q-table using the Q-learning equation\n",
    "        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "        \n",
    "        state = next_state  # Move to the next state\n",
    "\n",
    "# Print final Q-table\n",
    "print(\"Final Q-table:\")\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52210139-6af8-4002-b614-da7ff5a019e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Code for Deep Q-Learning (using TensorFlow/Keras):\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define the environment and actions as in the previous example\n",
    "states = 10  # Simulate continuous states (for simplicity, discretized)\n",
    "actions = 3  # 3 possible actions\n",
    "state_size = 10  # Number of state features\n",
    "action_size = 3  # Number of actions\n",
    "\n",
    "# Create a simple neural network model for Deep Q-Learning\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(64, input_dim=state_size, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(action_size, activation='linear')  # Q-values for each action\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Initialize model and memory\n",
    "model = create_model()\n",
    "memory = []  # List to store (state, action, reward, next_state) tuples\n",
    "\n",
    "# Q-learning hyperparameters\n",
    "gamma = 0.9  # discount factor\n",
    "epsilon = 0.1  # exploration rate (epsilon-greedy)\n",
    "batch_size = 32\n",
    "\n",
    "# Define a simple function to simulate the environment and train the model\n",
    "def train_model():\n",
    "    for episode in range(1000):\n",
    "        state = np.random.rand(state_size)  # Simulate a random state\n",
    "        \n",
    "        for time in range(100):\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.choice(actions)  # Explore: choose random action\n",
    "            else:\n",
    "                q_values = model.predict(state.reshape(1, -1))\n",
    "                action = np.argmax(q_values)  # Exploit: choose best action\n",
    "            \n",
    "            next_state = np.random.rand(state_size)  # Simulate the next state\n",
    "            reward = np.random.randn()  # Simulate a random reward\n",
    "            \n",
    "            memory.append((state, action, reward, next_state))\n",
    "            \n",
    "            # Train the model with a random batch from memory\n",
    "            if len(memory) > batch_size:\n",
    "                minibatch = np.random.choice(memory, batch_size)\n",
    "                for s, a, r, ns in minibatch:\n",
    "                    target = r + gamma * np.max(model.predict(ns.reshape(1, -1)))\n",
    "                    q_values = model.predict(s.reshape(1, -1))\n",
    "                    q_values[0][a] = target\n",
    "                    model.fit(s.reshape(1, -1), q_values, verbose=0)\n",
    "            \n",
    "            state = next_state  # Move to the next state\n",
    "\n",
    "train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6a8e6e-2238-43af-aae5-d5e003a4b26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Sample to Demonstrate Key Terminologies:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# States: Represent customer balances\n",
    "states = [0, 1, 2]  # 0: Low balance, 1: Medium balance, 2: High balance\n",
    "\n",
    "# Actions: What actions the bank can take\n",
    "actions = [0, 1, 2]  # 0: No action, 1: Offer loan, 2: Send notification\n",
    "\n",
    "# Initialize Q-table with zeros\n",
    "Q = np.zeros((len(states), len(actions)))\n",
    "\n",
    "# Define parameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration factor (epsilon-greedy)\n",
    "episodes = 1000  # Number of training episodes\n",
    "\n",
    "# Rewards for each action-state pair\n",
    "rewards = {\n",
    "    (0, 0): -1, (0, 1): -10, (0, 2): 2,  # Low balance\n",
    "    (1, 0): 0, (1, 1): 2, (1, 2): 1,   # Medium balance\n",
    "    (2, 0): 1, (2, 1): 5, (2, 2): 3   # High balance\n",
    "}\n",
    "\n",
    "# Simulate Q-learning process\n",
    "for episode in range(episodes):\n",
    "    state = np.random.choice(states)  # Start from a random state\n",
    "    \n",
    "    while state != 2:  # Continue until we reach 'high balance' state\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.choice(actions)  # Explore: choose random action\n",
    "        else:\n",
    "            action = np.argmax(Q[state])  # Exploit: choose action with max Q-value\n",
    "        \n",
    "        # Simulate reward and next state\n",
    "        next_state = np.random.choice(states)  # Transition to a new state (randomly)\n",
    "        reward = rewards[(state, action)]  # Get the reward for the state-action pair\n",
    "        \n",
    "        # Update Q-table using the Q-learning equation\n",
    "        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "        \n",
    "        state = next_state  # Transition to the next state\n",
    "\n",
    "# Print final Q-table\n",
    "print(\"Final Q-table:\")\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb924a7-1966-4580-b7bc-4d9457646c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Code for Determining Q-value in a Banking Scenario\n",
    "# Below is a # Python Code example for determining the Q-values in the ATM scenario described above.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define states: Represent customer balances (Low, Medium, High)\n",
    "states = [0, 1, 2]  # 0: Low balance, 1: Medium balance, 2: High balance\n",
    "\n",
    "# Define actions: What actions the bank can take\n",
    "actions = [0, 1, 2]  # 0: No action, 1: Offer loan, 2: Send notification\n",
    "\n",
    "# Initialize Q-table with zeros (3 states, 3 actions)\n",
    "Q = np.zeros((len(states), len(actions)))\n",
    "\n",
    "# Define learning parameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration factor (epsilon-greedy)\n",
    "episodes = 1000  # Number of training episodes\n",
    "\n",
    "# Define rewards for each action-state pair\n",
    "rewards = {\n",
    "    (0, 0): -1, (0, 1): -10, (0, 2): 2,  # Low balance state rewards\n",
    "    (1, 0): 0, (1, 1): 2, (1, 2): 1,   # Medium balance state rewards\n",
    "    (2, 0): 1, (2, 1): 5, (2, 2): 3   # High balance state rewards\n",
    "}\n",
    "\n",
    "# Simulate Q-learning process\n",
    "for episode in range(episodes):\n",
    "    state = np.random.choice(states)  # Start from a random state\n",
    "    \n",
    "    while state != 2:  # Continue until we reach 'high balance' state\n",
    "        # Exploration: Choose a random action, or Exploitation: Choose action with max Q-value\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.choice(actions)  # Explore: choose random action\n",
    "        else:\n",
    "            action = np.argmax(Q[state])  # Exploit: choose action with max Q-value\n",
    "        \n",
    "        # Simulate reward and next state\n",
    "        next_state = np.random.choice(states)  # Transition to a new state (randomly)\n",
    "        reward = rewards[(state, action)]  # Get the reward for the state-action pair\n",
    "        \n",
    "        # Update Q-table using the Q-learning update rule\n",
    "        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "        \n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "# Print final Q-table\n",
    "print(\"Final Q-table:\")\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7e97a7-56cb-46bd-9465-744dd4a5eb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing LangChain:\n",
    "# To install LangChain and the necessary dependencies, run the following commands:\n",
    "\n",
    "!pip install langchain openai\n",
    "!pip install pandas  # In case you want to use structured data like CSVs\n",
    "!pip install requests  # For API interaction\n",
    "# Basic LangChain Setup with # Python Code:\n",
    "# Let's start by using LangChain with an LLM from OpenAI (e.g., GPT-3 or GPT-4) in a simple chain.\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Initialize the OpenAI model (ensure you have your OpenAI API key)\n",
    "llm = OpenAI(api_key=\"your-openai-api-key\", temperature=0.7)\n",
    "\n",
    "# Create a prompt template\n",
    "template = \"Translate the following English text to French: {text}\"\n",
    "prompt = PromptTemplate(input_variables=[\"text\"], template=template)\n",
    "\n",
    "# Create the LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain\n",
    "output = chain.run(\"Hello, how are you?\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f93aafc-2d39-4e6e-8439-c5855389bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# : A bank's chatbot that answers customer queries about account balances, recent transactions, or loan information.\n",
    "\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.agents import Tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Example tools: Assume functions to interact with bank APIs are available\n",
    "def get_balance(account_id):\n",
    "    # Dummy function, replace with real API call to bank backend\n",
    "    return 1000.0  # Dummy balance\n",
    "\n",
    "def get_transaction_history(account_id):\n",
    "    # Dummy function, replace with real transaction query\n",
    "    return [\"Transaction 1: -$200\", \"Transaction 2: +$500\"]\n",
    "\n",
    "# Create tool for balance query\n",
    "balance_tool = Tool(\n",
    "    name=\"Get Account Balance\",\n",
    "    func=get_balance,\n",
    "    description=\"This tool retrieves the balance for a given account.\"\n",
    ")\n",
    "\n",
    "# Create tool for transaction history query\n",
    "transaction_tool = Tool(\n",
    "    name=\"Get Transaction History\",\n",
    "    func=get_transaction_history,\n",
    "    description=\"This tool retrieves the transaction history for a given account.\"\n",
    ")\n",
    "\n",
    "# Initialize the language model\n",
    "llm = OpenAI(api_key=\"your-openai-api-key\", temperature=0.5)\n",
    "\n",
    "# Create the agent with tools\n",
    "tools = [balance_tool, transaction_tool]\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")\n",
    "\n",
    "# Simulating customer query for balance\n",
    "query = \"What is my current account balance?\"\n",
    "response = agent.run(query)\n",
    "print(response)\n",
    "\n",
    "# Simulating customer query for transactions\n",
    "query = \"Show me my last 5 transactions.\"\n",
    "response = agent.run(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99ac41d-2508-4909-9ebd-3ec1f3577414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing Loan Applications\n",
    "# LangChain can automate the review and analysis of loan application forms, helping the bank make faster decisions \n",
    "# based on customer inputs. It can be used for document summarization, checking eligibility criteria, and auto-filling forms.\n",
    "# Scenario: Automatically review loan applications and extract key details such as the applicant's name, income, loan amount, etc.\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import AnalyzeDocumentChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Define a document loader to read loan application (e.g., a PDF file)\n",
    "loader = TextLoader(\"loan_application.txt\")\n",
    "\n",
    "# Use a prompt template to analyze the document content\n",
    "template = \"\"\"\n",
    "Extract the applicant's name, requested loan amount, and monthly income from the following loan application:\n",
    "{document}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"document\"], template=template)\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OpenAI(api_key=\"your-openai-api-key\", temperature=0.3)\n",
    "\n",
    "# Create the LLMChain for document analysis\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Load the document\n",
    "document = loader.load()\n",
    "\n",
    "# Run the analysis chain\n",
    "output = chain.run(document[0].page_content)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24900c24-2b97-4d9c-8902-c23aed16ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario: Automatically flag suspicious transactions based on specific patterns like large transfers, frequent international \n",
    "# transactions, or inconsistent spending.\n",
    "\n",
    "import pandas as pd\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Sample transaction data (in practice, this would come from the bank's transaction system)\n",
    "data = {\n",
    "    \"transaction_id\": [1, 2, 3, 4],\n",
    "    \"amount\": [1500, 25000, 50, 2000],\n",
    "    \"transaction_type\": [\"Debit\", \"Transfer\", \"Debit\", \"Transfer\"],\n",
    "    \"location\": [\"USA\", \"Switzerland\", \"USA\", \"Germany\"],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a prompt template to analyze transactions\n",
    "template = \"\"\"\n",
    "Analyze the following transactions for possible fraudulent activity based on amount, transaction type, and location:\n",
    "{transactions}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"transactions\"], template=template)\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OpenAI(api_key=\"your-openai-api-key\", temperature=0.6)\n",
    "\n",
    "# Create the analysis chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Convert the transaction data to a text format\n",
    "transactions_text = df.to_string(index=False)\n",
    "\n",
    "# Run the chain\n",
    "output = chain.run(transactions_text)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63ffd16-a17d-43df-a27d-0cba13b8e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario: Automating responses to frequently asked questions like checking account balances, transferring money, or understanding loan terms.\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "\n",
    "# Function to simulate querying a customer account balance\n",
    "def get_balance(account_id):\n",
    "    # Simulating API response; in practice, this would query a banking API\n",
    "    return f\"Your current balance is $10,000 for account {account_id}.\"\n",
    "\n",
    "# Create a tool for the agent to use for balance queries\n",
    "balance_tool = Tool(\n",
    "    name=\"Balance Query\",\n",
    "    func=get_balance,\n",
    "    description=\"This tool helps in retrieving customer account balances.\"\n",
    ")\n",
    "\n",
    "# Initialize the LLM (OpenAI GPT-3/4 model)\n",
    "llm = OpenAI(api_key=\"your-openai-api-key\", temperature=0.5)\n",
    "\n",
    "# Create an agent using LangChain's zero-shot model (reactive agent)\n",
    "tools = [balance_tool]\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")\n",
    "\n",
    "# Simulate a customer query\n",
    "query = \"What is my balance in account 12345?\"\n",
    "response = agent.run(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56800c4-19de-4110-b395-d89662942f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario: Extracting applicant details from loan application forms (like PDFs), checking eligibility, and providing recommendations.\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Simulating a loan application document (in reality, you'd load from a file)\n",
    "application_text = \"\"\"\n",
    "Applicant: John Doe\n",
    "Income: $75,000\n",
    "Requested Loan: $20,000\n",
    "Credit Score: 720\n",
    "\"\"\"\n",
    "\n",
    "# Creating a prompt template to extract loan details\n",
    "template = \"\"\"\n",
    "Extract the applicant's name, requested loan amount, and income from the following loan application:\n",
    "{document}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"document\"], template=template)\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OpenAI(api_key=\"your-openai-api-key\", temperature=0.3)\n",
    "\n",
    "# Create the chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the application analysis\n",
    "output = chain.run(application_text)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaabe591-4e89-4bed-9fdf-6db923f7a7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# : Flagging suspicious transactions based on transaction amount, type, and location.\n",
    "\n",
    "import pandas as pd\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Example transaction data (this would typically come from the bank's internal system)\n",
    "data = {\n",
    "    \"transaction_id\": [1, 2, 3, 4],\n",
    "    \"amount\": [1500, 25000, 50, 2000],\n",
    "    \"transaction_type\": [\"Debit\", \"Transfer\", \"Debit\", \"Transfer\"],\n",
    "    \"location\": [\"USA\", \"Switzerland\", \"USA\", \"Germany\"],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Creating a prompt to analyze transactions for potential fraud\n",
    "template = \"\"\"\n",
    "Analyze the following transactions and identify any that may be considered suspicious due to high amounts or unusual locations:\n",
    "{transactions}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"transactions\"], template=template)\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = OpenAI(api_key=\"your-openai-api-key\", temperature=0.6)\n",
    "\n",
    "# Create the analysis chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Convert the transaction data to a text format for the model\n",
    "transactions_text = df.to_string(index=False)\n",
    "\n",
    "# Run the chain to analyze the transactions\n",
    "output = chain.run(transactions_text)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa3c74a-edd5-4b2d-8a4b-43b0a0abec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario: Offering personalized advice for savings and investments based on a customers financial profile.\n",
    "\n",
    "# Example customer data\n",
    "customer_data = {\n",
    "    \"income\": 5000,\n",
    "    \"expenses\": 3000,\n",
    "    \"savings\": 20000,\n",
    "    \"investment_preferences\": \"low risk\",\n",
    "}\n",
    "\n",
    "# Creating a prompt template for financial advice\n",
    "template = \"\"\"\n",
    "Given the following financial profile, provide personalized advice on savings and investments:\n",
    "Income: {income}\n",
    "Expenses: {expenses}\n",
    "Savings: {savings}\n",
    "Investment Preferences: {investment_preferences}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"income\", \"expenses\", \"savings\", \"investment_preferences\"], template=template)\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = OpenAI(api_key=\"your-openai-api-key\", temperature=0.7)\n",
    "\n",
    "# Create the chain for generating personalized advice\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain with customer data\n",
    "output = chain.run(customer_data)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2341370e-b810-4900-ae16-5d170b61bade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banking Scenario: Automating customer support for account-related queries by leveraging an LLM to generate responses to frequently asked questions.\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Initialize the LLM (OpenAI GPT-3)\n",
    "llm = OpenAI(api_key=\"your-openai-api-key\", temperature=0.7)\n",
    "\n",
    "# Create a prompt template for frequently asked questions\n",
    "template = \"What is the current balance of account {account_number}?\"\n",
    "prompt = PromptTemplate(input_variables=[\"account_number\"], template=template)\n",
    "\n",
    "# Create the LLM chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Simulate querying the account balance\n",
    "response = chain.run(account_number=\"123456\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206a56fa-5e48-45e7-87ef-444f4354babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banking Scenario: Creating a loan processing system that checks a customer's credit score, loan eligibility, and then generates a report.\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OpenAI(api_key=\"your-openai-api-key\", temperature=0.7)\n",
    "\n",
    "# Define templates for different stages of loan processing\n",
    "credit_score_template = \"What is the credit score for a person with the following information: {customer_data}?\"\n",
    "loan_eligibility_template = \"Is a person eligible for a loan based on the following credit score: {credit_score}?\"\n",
    "\n",
    "# Create the chains for processing the loan application\n",
    "credit_score_chain = LLMChain(llm=llm, prompt=PromptTemplate(input_variables=[\"customer_data\"], template=credit_score_template))\n",
    "loan_eligibility_chain = LLMChain(llm=llm, prompt=PromptTemplate(input_variables=[\"credit_score\"], template=loan_eligibility_template))\n",
    "\n",
    "# Simulate customer data\n",
    "customer_data = {\"name\": \"John Doe\", \"income\": 60000, \"loan_amount\": 20000}\n",
    "\n",
    "# Run the chains sequentially\n",
    "credit_score = credit_score_chain.run(customer_data)\n",
    "eligibility = loan_eligibility_chain.run(credit_score)\n",
    "\n",
    "print(f\"Credit Score: {credit_score}\")\n",
    "print(f\"Loan Eligibility: {eligibility}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c71c295-2690-4aa5-9596-feb1457c502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banking Scenario: Building a virtual assistant that can help customers with various services, such as checking account balance, \n",
    "# transferring funds, or providing loan details.\n",
    "\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.agents import Tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Define a few tools for handling bank operations\n",
    "def get_balance(account_id):\n",
    "    # Placeholder: In practice, integrate with your bank's system\n",
    "    return f\"Your balance for account {account_id} is $5000.\"\n",
    "\n",
    "def transfer_funds(from_account, to_account, amount):\n",
    "    # Placeholder: Simulate a funds transfer\n",
    "    return f\"Transferred ${amount} from account {from_account} to account {to_account}.\"\n",
    "\n",
    "# Initialize tools\n",
    "balance_tool = Tool(name=\"Balance\", func=get_balance, description=\"Fetch account balance.\")\n",
    "transfer_tool = Tool(name=\"Transfer\", func=transfer_funds, description=\"Transfer funds between accounts.\")\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = OpenAI(api_key=\"your-openai-api-key\", temperature=0.5)\n",
    "\n",
    "# Initialize the agent with the tools\n",
    "tools = [balance_tool, transfer_tool]\n",
    "agent = initialize_agent(tools, llm, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Example user query\n",
    "query = \"What is the balance of account 12345?\"\n",
    "response = agent.run(query)\n",
    "print(response)\n",
    "\n",
    "query = \"Transfer $500 from account 12345 to account 67890.\"\n",
    "response = agent.run(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e278fd-8f8c-4870-b2e4-b778a768b481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banking Scenario: Building a virtual banking assistant that remembers past transactions or loan requests to provide more personalized service.\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize memory to store conversation history\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OpenAI(api_key=\"your-openai-api-key\", temperature=0.7)\n",
    "\n",
    "# Create a prompt template to respond based on memory\n",
    "template = \"What was the last transaction made by {user_name}? {conversation_history}\"\n",
    "prompt = PromptTemplate(input_variables=[\"user_name\", \"conversation_history\"], template=template)\n",
    "\n",
    "# Create the LLM chain with memory\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Simulate a series of interactions\n",
    "conversation = [\n",
    "    \"What is my balance?\",\n",
    "    \"Your balance is $5,000.\",\n",
    "    \"Have I made any recent transactions?\",\n",
    "    \"Yes, you made a $200 purchase yesterday.\"\n",
    "]\n",
    "\n",
    "# Store conversation history in memory\n",
    "for message in conversation:\n",
    "    memory.chat_memory.add_user_message(message)\n",
    "\n",
    "# Generate response based on memory\n",
    "response = chain.run(user_name=\"John\", conversation_history=memory.chat_memory.get_history())\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8259bad6-e338-41c5-b170-138a690b4455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banking Scenario: Integrating an external credit score API to check customer credit ratings for loan approval.\n",
    "\n",
    "import requests\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import Tool\n",
    "\n",
    "# Simulating an external credit score API (replace with real API in production)\n",
    "def get_credit_score(customer_id):\n",
    "    # Dummy response; replace with real API call\n",
    "    return 720  # A valid credit score for the demo\n",
    "\n",
    "# Tool to interact with the credit score API\n",
    "credit_score_tool = Tool(\n",
    "    name=\"Credit Score Checker\",\n",
    "    func=get_credit_score,\n",
    "    description=\"Fetch the credit score of a customer.\"\n",
    ")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OpenAI(api_key=\"your-openai-api-key\", temperature=0.7)\n",
    "\n",
    "# Create the agent with the tool\n",
    "tools = [credit_score_tool]\n",
    "agent = initialize_agent(tools, llm, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Simulate customer query to check credit score\n",
    "query = \"What is the credit score of customer 123?\"\n",
    "response = agent.run(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d276066-c5ed-45fc-a400-6d3c093d533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banking Scenario: Automating the extraction of key information from loan application documents (e.g., name, loan amount, income) for faster processing.\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Simulating a loan application text document\n",
    "application_text = \"\"\"\n",
    "Applicant: Jane Doe\n",
    "Loan Amount: $30,000\n",
    "Income: $85,000\n",
    "\"\"\"\n",
    "\n",
    "# Define a prompt to extract details from the application\n",
    "template = \"\"\"\n",
    "Extract the following details from the loan application:\n",
    "1. Applicant Name\n",
    "2. Requested Loan Amount\n",
    "3. Income\n",
    "{document}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"document\"], template=template)\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OpenAI(api_key=\"your-openai-api-key\", temperature=0.5)\n",
    "\n",
    "# Create LLM chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain with the application document\n",
    "output = chain.run(application_text)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65f6c58-7056-45dd-9c06-cdc5ca451c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banking Scenario: A virtual assistant for customers in the banking sector that can answer questions about account balances,\n",
    "# loan status, and recent transactions.\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Initialize OpenAI LLM (GPT-3/4) for querying\n",
    "llm = OpenAI(api_key=\"your-openai-api-key\", temperature=0.7)\n",
    "\n",
    "# Define the prompt template for answering customer queries\n",
    "template = \"What is the balance of account {account_number}?\"\n",
    "prompt = PromptTemplate(input_variables=[\"account_number\"], template=template)\n",
    "\n",
    "# Create an LLM chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Example customer query\n",
    "response = chain.run(account_number=\"12345\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bd164e-4be5-4141-9920-74dc3f04f3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banking Scenario: A banking application that integrates with a credit score API to check a customer's eligibility for a loan.\n",
    "\n",
    "import requests\n",
    "from langchain.agents import Tool, initialize_agent, AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Simulated external API for checking credit score\n",
    "def check_credit_score(customer_id):\n",
    "    # Simulating an API response, replace with a real API in production\n",
    "    return 750  # Example of a good credit score\n",
    "\n",
    "# Tool for checking credit score\n",
    "credit_score_tool = Tool(\n",
    "    name=\"Credit Score Checker\",\n",
    "    func=check_credit_score,\n",
    "    description=\"Check the credit score of a customer.\"\n",
    ")\n",
    "\n",
    "# Initialize OpenAI LLM\n",
    "llm = OpenAI(api_key=\"your-openai-api-key\", temperature=0.6)\n",
    "\n",
    "# Initialize agent with credit score tool\n",
    "tools = [credit_score_tool]\n",
    "agent = initialize_agent(tools, llm, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Example query to check credit score\n",
    "response = agent.run(\"What is the credit score for customer 12345?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dd4e2f-e3be-4880-a15f-95ee172eb0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banking Scenario: A virtual assistant that remembers a customer's previous loan application details, such as loan amount and current status.\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize memory to store conversation history\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Initialize OpenAI LLM\n",
    "llm = OpenAI(api_key=\"your-openai-api-key\", temperature=0.7)\n",
    "\n",
    "# Define a prompt template that uses conversation history\n",
    "template = \"Based on the following conversation, what is the loan status for the customer? {conversation_history}\"\n",
    "prompt = PromptTemplate(input_variables=[\"conversation_history\"], template=template)\n",
    "\n",
    "# Create LLM chain with memory\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Simulate a series of interactions\n",
    "conversation = [\n",
    "    \"What is the loan status for my application?\",\n",
    "    \"Your loan application is being processed.\",\n",
    "    \"What is the loan amount I applied for?\",\n",
    "    \"You applied for a $20,000 loan.\"\n",
    "]\n",
    "\n",
    "# Store conversation history in memory\n",
    "for message in conversation:\n",
    "    memory.chat_memory.add_user_message(message)\n",
    "\n",
    "# Generate response based on memory\n",
    "response = chain.run(conversation_history=memory.chat_memory.get_history())\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280198c4-5b69-47b4-ab94-12e7f4ed4e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banking Scenario: Automatically extracting key details from a loan application document (e.g., applicant name, loan amount, income).\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Simulating a loan application document\n",
    "application_text = \"\"\"\n",
    "Applicant: Jane Doe\n",
    "Loan Amount: $50,000\n",
    "Income: $85,000\n",
    "\"\"\"\n",
    "\n",
    "# Define a prompt template to extract relevant information from the application\n",
    "template = \"\"\"\n",
    "Extract the following details from the loan application:\n",
    "1. Applicant Name\n",
    "2. Loan Amount\n",
    "3. Income\n",
    "{document}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"document\"], template=template)\n",
    "\n",
    "# Initialize OpenAI LLM\n",
    "llm = OpenAI(api_key=\"your-openai-api-key\", temperature=0.5)\n",
    "\n",
    "# Create LLM chain to process the loan application document\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain with the loan application document\n",
    "output = chain.run(application_text)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9197a9-1e71-4a77-bc7c-84918bb17c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banking Scenario: Automatically retrieving the latest stock market information or interest rates from external websites.\n",
    "\n",
    "from langchain.tools import SeleniumWebBrowser\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import Tool, initialize_agent, AgentType\n",
    "\n",
    "# Example tool for web scraping using Selenium\n",
    "def get_stock_price(stock_symbol):\n",
    "    browser = SeleniumWebBrowser(driver_path=\"path_to_your_webdriver\")\n",
    "    browser.get(f'https://www.example.com/stock/{stock_symbol}')\n",
    "    price = browser.find_element_by_id('stock_price').text\n",
    "    browser.quit()\n",
    "    return price\n",
    "\n",
    "# Create a tool for the agent to use\n",
    "stock_price_tool = Tool(\n",
    "    name=\"Stock Price Fetcher\",\n",
    "    func=get_stock_price,\n",
    "    description=\"Fetch the current stock price for a given symbol.\"\n",
    ")\n",
    "\n",
    "# Initialize LLM and agent\n",
    "llm = OpenAI(api_key=\"your-openai-api-key\", temperature=0.6)\n",
    "tools = [stock_price_tool]\n",
    "agent = initialize_agent(tools, llm, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Example query to get stock price for a symbol\n",
    "response = agent.run(\"What is the stock price for AAPL?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca61fa8-2a0b-429b-808d-cef1361c41f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banking Scenario: Querying a customer database to retrieve account details or transaction history.\n",
    "\n",
    "import sqlite3\n",
    "from langchain.agents import Tool, initialize_agent, AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Example database query function (SQLite for demonstration)\n",
    "def get_transaction_history(account_id):\n",
    "    conn = sqlite3.connect('banking_data.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT * FROM transactions WHERE account_id=?\", (account_id,))\n",
    "    transactions = cursor.fetchall()\n",
    "    conn.close()\n",
    "    return transactions\n",
    "\n",
    "# Tool for querying transaction history\n",
    "transaction_tool = Tool(\n",
    "    name=\"Transaction History Fetcher\",\n",
    "    func=get_transaction_history,\n",
    "    description=\"Fetch transaction history for a given account.\"\n",
    ")\n",
    "\n",
    "# Initialize LLM and agent\n",
    "llm = OpenAI(api_key=\"your-openai-api-key\", temperature=0.6)\n",
    "tools = [transaction_tool]\n",
    "agent = initialize_agent(tools, llm, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Example query to fetch transaction history for account 12345\n",
    "response = agent.run(\"What are the last 5 transactions for account 12345?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563262f2-d811-4318-810b-877a39ad1595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banking Scenario: Storing loan approval documents or customer correspondence in cloud storage (e.g., S3) for future access.\n",
    "\n",
    "import boto3\n",
    "from langchain.tools import Tool\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "# Simulated function for retrieving documents from S3\n",
    "def get_loan_approval_document(document_id):\n",
    "    s3 = boto3.client('s3')\n",
    "    bucket_name = 'banking-docs'\n",
    "    file_name = f\"loan_approval_{document_id}.pdf\"\n",
    "    file = s3.get_object(Bucket=bucket_name, Key=file_name)\n",
    "    return file['Body'].read().decode('utf-8')\n",
    "\n",
    "# Tool for interacting with file storage\n",
    "file_tool = Tool(\n",
    "    name=\"File Storage Handler\",\n",
    "    func=get_loan_approval_document,\n",
    "    description=\"Fetch loan approval documents from cloud storage.\"\n",
    ")\n",
    "\n",
    "# Initialize LLM and agent\n",
    "llm = OpenAI(api_key=\"your-openai-api-key\", temperature=0.7)\n",
    "tools = [file_tool]\n",
    "agent = initialize_agent(tools, llm, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Example query to fetch loan approval document\n",
    "response = agent.run(\"Retrieve the loan approval document with ID 12345\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d20f8d-5a4a-4702-aaca-fdf5fb541e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Prompt Template:\n",
    "# In a banking scenario, let's create a prompt template to answer a customer's query about their loan application status.\n",
    "# Example: We want to create a prompt that fetches the loan status for a specific customer. The template would look like this:\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define the template for the loan application status prompt\n",
    "loan_status_template = \"\"\"\n",
    "You are a virtual assistant for a bank. Your task is to check the loan application status.\n",
    "Check the loan status for customer with account number {account_number} and provide a detailed response.\n",
    "\"\"\"\n",
    "# Here, {account_number} is a placeholder that will be replaced with the customer's account number.\n",
    "# ________________________________________\n",
    "# 2. Set Up Input Variables:\n",
    "# For the banking scenario, the input variable will be the account_number that we will pass dynamically \n",
    "# based on the user's request. The customer might provide their account number, and we will fetch the status accordingly.\n",
    "\n",
    "# Example input variable for account number\n",
    "account_number = \"123456789\"\n",
    "# This value would be dynamically replaced in the prompt template to generate a query that checks the loan status for the customer.\n",
    "# ________________________________________\n",
    "# 3. Create Prompt with LangChain's PromptTemplate Class:\n",
    "# Once you define the template and the input variables, you can use LangChain's PromptTemplate class to create a complete prompt.\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define the template with a dynamic placeholder for account number\n",
    "loan_status_template = \"\"\"\n",
    "You are a virtual assistant for a bank. Your task is to check the loan application status.\n",
    "Check the loan status for customer with account number {account_number} and provide a detailed response.\n",
    "\"\"\"\n",
    "\n",
    "# Create the PromptTemplate object with the input variable (account_number)\n",
    "prompt = PromptTemplate(input_variables=[\"account_number\"], template=loan_status_template)\n",
    "\n",
    "# Render the prompt with a specific account number\n",
    "rendered_prompt = prompt.render(account_number=\"123456789\")\n",
    "print(rendered_prompt)\n",
    "\n",
    "\n",
    "# 4. Use the Prompt with an LLM:\n",
    "# Next, we use the rendered_prompt with an LLM, like OpenAI's GPT-3, to generate the response.\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize OpenAI LLM\n",
    "llm = OpenAI(api_key=\"your-openai-api-key\", temperature=0.7)\n",
    "\n",
    "# Generate a response based on the rendered prompt\n",
    "response = llm(rendered_prompt)\n",
    "print(response)\n",
    "\n",
    "# 5. Combine Prompts with Chains (Optional):\n",
    "# In more complex scenarios, you might want to combine multiple prompts and steps to build a comprehensive system. \n",
    "# LangChain allows you to chain multiple steps together. For instance, you can first fetch account details from a database, \n",
    "# then pass the data to the LLM to generate a detailed response.\n",
    "# Example: Combining Account Query with Loan Status Response\n",
    "# Lets imagine a scenario where we query a database for a customers loan application and then provide a detailed loan status based on the results.\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "import sqlite3\n",
    "\n",
    "# Simulate a simple database to fetch loan application status\n",
    "def fetch_loan_application(account_number):\n",
    "    # This is a mock function simulating fetching loan data from a database\n",
    "    # In a real application, you would query your actual database here.\n",
    "    mock_data = {\n",
    "        \"123456789\": \"In Progress\",\n",
    "        \"987654321\": \"Approved\",\n",
    "        \"112233445\": \"Rejected\"\n",
    "    }\n",
    "    return mock_data.get(account_number, \"No Application Found\")\n",
    "\n",
    "# Define the template with a placeholder for loan status\n",
    "loan_status_template = \"\"\"\n",
    "You are a virtual assistant for a bank. Your task is to check the loan application status.\n",
    "The status of the loan for customer with account number {account_number} is {loan_status}.\n",
    "Please provide a detailed response based on the loan status.\n",
    "\"\"\"\n",
    "\n",
    "# Create the PromptTemplate object with the input variables\n",
    "prompt = PromptTemplate(input_variables=[\"account_number\", \"loan_status\"], template=loan_status_template)\n",
    "\n",
    "# Create a chain to integrate the database query and prompt\n",
    "llm = OpenAI(api_key=\"your-openai-api-key\", temperature=0.7)\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Function to fetch loan status and pass it to the prompt\n",
    "def get_loan_status(account_number):\n",
    "    loan_status = fetch_loan_application(account_number)  # Fetch loan status from \"database\"\n",
    "    result = chain.run(account_number=account_number, loan_status=loan_status)  # Pass it to LLM\n",
    "    return result\n",
    "\n",
    "# Get loan status for account 123456789\n",
    "response = get_loan_status(\"123456789\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344910a7-d70c-4db9-9a8d-f4243866f8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# . Set Up the Environment\n",
    "# Before you start building with LangChain, you need to install the required packages. In most cases, this will include \n",
    "# LangChain itself, and other dependencies like OpenAI (or other LLM providers), database connectors, or API clients.\n",
    "\n",
    "!pip install langchain openai\n",
    "# For the banking application, you may need additional libraries, such as for connecting to a database (e.g., sqlite3, pymysql),\n",
    "# or for interacting with APIs (e.g., requests, boto3 for AWS integration).\n",
    "# ________________________________________\n",
    "# 2. Create and Configure Prompts\n",
    "# The first step in using LangChain is to define prompts that will guide the behavior of the LLM. These prompts are essential in \n",
    "# instructing the model on how to respond to user queries.\n",
    "# Banking Scenario: Lets build a prompt for checking a customer's loan status using a dynamic account number.\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define the prompt template for loan status query\n",
    "loan_status_template = \"\"\"\n",
    "You are a virtual assistant for a bank. Your task is to check the loan application status.\n",
    "Check the loan status for customer with account number {account_number} and provide a detailed response.\n",
    "\"\"\"\n",
    "\n",
    "# Create the PromptTemplate object\n",
    "prompt = PromptTemplate(input_variables=[\"account_number\"], template=loan_status_template)\n",
    "# Here, the {account_number} will be replaced by an actual account number dynamically when the prompt is invoked.\n",
    "# ________________________________________\n",
    "# 3. Integrate External Tools and Data Sources\n",
    "# LangChain shines when it comes to integrating external tools, databases, and APIs. In banking applications, you will often need to query databases for customer data or interact with external APIs for services like credit score checking or stock market information.\n",
    "# Banking Scenario: Lets simulate querying a simple customer database for loan application details.\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "# Simulate a database function that fetches loan status based on account number\n",
    "def fetch_loan_status(account_number):\n",
    "    conn = sqlite3.connect('banking_data.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT loan_status FROM loans WHERE account_number=?\", (account_number,))\n",
    "    result = cursor.fetchone()\n",
    "    conn.close()\n",
    "    return result[0] if result else \"No application found\"\n",
    "\n",
    "# Example query to fetch loan status for a specific customer\n",
    "account_number = \"123456789\"\n",
    "loan_status = fetch_loan_status(account_number)\n",
    "print(loan_status)  # \"Approved\" or \"Pending\", etc.\n",
    "# In this example, we connect to a database to retrieve the loan status of a customer based on their account number.\n",
    "# LangChain can be integrated with such tools to automate the flow of fetching data and generating responses.\n",
    "# ________________________________________\n",
    "# 4. Use Memory for Contextual Awareness (Optional)\n",
    "# In more sophisticated applications, you may want the model to \"remember\" information from previous interactions with the user. \n",
    "# LangChain provides memory integrations that allow LLMs to retain contextual information across sessions.\n",
    "# Banking Scenario: A customer might interact with a virtual assistant multiple times about different aspects of their \n",
    "# loan application. The assistant could \"remember\" previous interactions, such as the customers application status.\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize memory to store the conversation history\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Initialize the LLM (OpenAI GPT in this case)\n",
    "llm = OpenAI(api_key=\"your-openai-api-key\", temperature=0.7)\n",
    "\n",
    "# Define a prompt template for querying loan status\n",
    "template = \"What is the loan status for customer {account_number}? Please summarize the previous interactions.\"\n",
    "prompt = PromptTemplate(input_variables=[\"account_number\"], template=template)\n",
    "\n",
    "# Create a chain with memory\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Simulate a conversation\n",
    "memory.chat_memory.add_user_message(\"What is the status of my loan?\")\n",
    "memory.chat_memory.add_assistant_message(\"Your loan is under review.\")\n",
    "memory.chat_memory.add_user_message(\"When will I know the result?\")\n",
    "\n",
    "# Generate a response based on the memory and the account number\n",
    "response = chain.run(account_number=\"123456789\")\n",
    "print(response)\n",
    "\n",
    "# The assistant will be able to remember previous interactions and provide more context-aware answers.\n",
    "# ________________________________________\n",
    "# 5. Develop the Core Application Logic\n",
    "# Once you've set up your prompts, tools, and optional memory features, you can develop the core application logic. \n",
    "# This involves combining everything together to build the final solution, integrating database queries, external APIs, \n",
    "# and prompts to create intelligent interactions.\n",
    "# Banking Scenario: Lets build a banking chatbot that checks loan application status and provides the current credit \n",
    "# score using a simple integration with a mock external API.\n",
    "\n",
    "import requests\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import Tool, initialize_agent, AgentType\n",
    "\n",
    "# Simulate an external API for checking credit score\n",
    "def get_credit_score(account_number):\n",
    "    # Simulating an API request to check credit score\n",
    "    # In reality, this would be a call to an actual service.\n",
    "    credit_scores = {\"123456789\": 750, \"987654321\": 620}\n",
    "    return credit_scores.get(account_number, \"No score available\")\n",
    "\n",
    "# Define tools for querying loan status and credit score\n",
    "credit_score_tool = Tool(\n",
    "    name=\"Credit Score Fetcher\",\n",
    "    func=get_credit_score,\n",
    "    description=\"Fetch the credit score of a customer\"\n",
    ")\n",
    "\n",
    "# Initialize OpenAI LLM\n",
    "llm = OpenAI(api_key=\"your-openai-api-key\", temperature=0.7)\n",
    "\n",
    "# Create a chain that can call multiple tools\n",
    "tools = [credit_score_tool]\n",
    "agent = initialize_agent(tools, llm, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Get customer details and use the agent to provide a response\n",
    "account_number = \"123456789\"\n",
    "response = agent.run(f\"What is the loan status and credit score for customer {account_number}?\")\n",
    "print(response)\n",
    "\n",
    "# In this scenario, we combine querying a database for the loan status and using an external API to fetch the credit score. \n",
    "# The LLM provides a comprehensive response to the user based on both tools.\n",
    "# ________________________________________\n",
    "# 6. Implement Error Handling and Robustness\n",
    "# Error handling is essential for any real-world application. You should ensure that your LangChain-based system can handle \n",
    "# exceptions gracefully and provide meaningful feedback.\n",
    "# Example:\n",
    "\n",
    "def fetch_loan_status_with_error_handling(account_number):\n",
    "    try:\n",
    "        return fetch_loan_status(account_number)\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred while fetching the loan status: {str(e)}\"\n",
    "\n",
    "# Handling an invalid account number\n",
    "response = fetch_loan_status_with_error_handling(\"invalid_account\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45addb3-f2d5-4340-95cd-b1a217992e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banking Scenario: Fraud Detection: Lets assume a banking use case where we are trying to detect fraudulent transactions.\n",
    "# Well use a simple deep learning model to classify transactions as either fraudulent or non-fraudulent.\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Build a simple neural network for fraud detection\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Dense(64, input_dim=30, activation='relu'))  # Assume 30 features (e.g., transaction data)\n",
    "\n",
    "# Hidden layers\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout to prevent overfitting\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, activation='sigmoid'))  # Binary classification (fraud or not)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# In this example, we define a simple feed-forward neural network to classify transactions. The input_dim=30 corresponds to \n",
    "# 30 features describing a transaction (e.g., amount, time, merchant).\n",
    "# ________________________________________\n",
    "# 2. Training the Model\n",
    "# After building the model, we need to train it using labeled data. Keras provides an easy interface to train the model using the fit() function.\n",
    "# Banking Scenario: Training a Fraud Detection Model: Assume that we have a dataset containing both fraudulent and non-fraudulent transactions.\n",
    "\n",
    "# Example dataset: X is the feature matrix, y is the target labels (0 for non-fraud, 1 for fraud)\n",
    "X_train = ...\n",
    "y_train = ...\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "# Here, epochs=10 means the model will iterate over the data 10 times, and batch_size=32 means the model will update weights \n",
    "# after processing 32 samples at once. The validation_split=0.2 argument means 20% of the data will be used for validation during training.\n",
    "# ________________________________________\n",
    "# 3. Model Evaluation\n",
    "# Once the model is trained, its essential to evaluate its performance on unseen data using metrics like accuracy, precision, recall, F1 score, etc.\n",
    "# Banking Scenario: Evaluate the Performance of Fraud Detection Model: Lets assume that we have test data (X_test, y_test) \n",
    "# that the model has never seen.\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "# Evaluating the model on test data gives us an understanding of its performance and generalization ability to new, unseen data.\n",
    "# ________________________________________\n",
    "# 4. Prediction\n",
    "# After training and evaluating the model, you can use it to make predictions on new, unseen data. In a banking scenario, we can use the model to predict if a transaction is fraudulent or not.\n",
    "# Banking Scenario: Predict Fraudulent Transactions\n",
    "\n",
    "# New transaction data to predict (e.g., features of a transaction)\n",
    "new_transaction = ...\n",
    "\n",
    "# Predict if the transaction is fraudulent or not\n",
    "prediction = model.predict(new_transaction)\n",
    "\n",
    "# 0 indicates non-fraud, 1 indicates fraud\n",
    "if prediction[0] > 0.5:\n",
    "    print(\"Fraudulent transaction detected!\")\n",
    "else:\n",
    "    print(\"Transaction is legitimate.\")\n",
    "# Here, model.predict() outputs a probability, and we can classify the transaction as fraudulent if the probability exceeds 0.5.\n",
    "# ________________________________________\n",
    "# 5. Transfer Learning\n",
    "# Keras also supports transfer learning, where you can take a pre-trained model (on a large dataset) and fine-tune \n",
    "# it for a specific task with a smaller dataset. This is particularly useful when you have limited data but still want to \n",
    "# leverage pre-trained models.\n",
    "# Banking Scenario: Customer Churn Prediction\n",
    "# You can use a pre-trained model on customer behavior (e.g., from a general dataset) and fine-tune it for customer \n",
    "# churn prediction, a common task in banking.\n",
    "\n",
    "from keras.applications import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "# Load a pre-trained VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add custom layers for our specific task (e.g., churn prediction)\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create the model\n",
    "churn_model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Freeze the base model layers (keep pre-trained weights intact)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "churn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fine-tune the model with your churn data\n",
    "churn_model.fit(X_churn_train, y_churn_train, epochs=5)\n",
    "# In this scenario, we leverage VGG16, a pre-trained image model, and add custom layers for predicting customer churn.\n",
    "# ________________________________________\n",
    "# 6. Saving and Loading Models\n",
    "# Once a model is trained, its important to save it so you can deploy it without retraining. Keras allows you to save \n",
    "# models in both the JSON and HDF5 formats.\n",
    "# Banking Scenario: Saving and Loading a Trained Model\n",
    "\n",
    "# Save the model\n",
    "model.save(\"fraud_detection_model.h5\")\n",
    "\n",
    "# Load the saved model\n",
    "from keras.models import load_model\n",
    "loaded_model = load_model(\"fraud_detection_model.h5\")\n",
    "\n",
    "# Use the loaded model to make predictions\n",
    "loaded_model.predict(new_transaction)\n",
    "# Saving and loading models is useful for deploying models to production and avoiding unnecessary retraining.\n",
    "# ________________________________________\n",
    "# 7. Data Preprocessing and Augmentation\n",
    "# Keras provides tools to preprocess and augment data, especially for tasks like image classification, text analysis, \n",
    "# and more. In banking applications, preprocessing may involve normalization, missing value imputation, or encoding categorical variables.\n",
    "# Banking Scenario: Preprocessing Transaction Data\n",
    "\n",
    "from keras.preprocessing import StandardScaler\n",
    "\n",
    "# Normalize transaction data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Optionally, perform data augmentation (for example, for time-series data)\n",
    "# Data preprocessing is critical to ensure that the model performs well. For example, you might normalize the transaction amount,\n",
    "# time, and other numerical features to ensure uniformity and stability in training.\n",
    "# ________________________________________\n",
    "# 8. Advanced Features and Custom Layers\n",
    "# Keras also supports custom layers, loss functions, and other advanced features that allow you to tailor the model architecture \n",
    "# to your needs. This can be useful in specialized banking applications, like implementing custom risk scoring systems or building \n",
    "# hybrid models that combine machine learning and rule-based approaches.\n",
    "# Banking Scenario: Custom Loss Function for Risk Assessment\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "# Define a custom loss function that penalizes false negatives (fraud detection)\n",
    "def custom_loss(y_true, y_pred):\n",
    "    loss = binary_crossentropy(y_true, y_pred)\n",
    "    penalty = K.sum(K.cast(K.less(y_pred, 0.5), K.floatx()) * 0.5)  # False negative penalty\n",
    "    return loss + penalty\n",
    "\n",
    "# Compile the model with the custom loss\n",
    "model.compile(loss=custom_loss, optimizer='adam', metrics=['accuracy'])\n",
    "# In this example, we create a custom loss function that adds a penalty for false negatives, ensuring the model is more sensitive to detecting fraud.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46944f8-5d8d-4fdb-a578-59f9c039a0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banking Scenario: Fraud Detection\n",
    "# Lets assume a banking business scenario where we need to build a model to detect fraudulent transactions based on \n",
    "# features like the transaction amount, merchant, time, and customer details.\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Define a simple Sequential model for fraud detection\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Dense(64, input_dim=30, activation='relu'))  # 30 features from customer transaction data\n",
    "\n",
    "# Hidden layers\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout layer to prevent overfitting\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, activation='sigmoid'))  # Binary classification (fraud or not)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdf574c-0cd4-4926-a548-65cef454f409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Compilation\n",
    "# After defining the architecture, the next step is to compile the model. This involves specifying the optimizer, loss function, and metrics.\n",
    "# \tOptimizer: Adjusts the model weights based on the gradients of the loss function.\n",
    "# \tLoss Function: Measures the difference between the predicted and true values. For binary classification, we use binary_crossentropy.\n",
    "# \tMetrics: Used to track the model's performance during training. Common metrics include accuracy.\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "In this case, we use the Adam optimizer because it is well-suited for many deep learning problems. The binary_crossentropy loss function is appropriate since we are solving a binary classification problem (fraud or not).\n",
    "________________________________________\n",
    "3. Model Training\n",
    "After compiling the model, the next step is to train the model. This involves providing the model with training data and letting it learn by adjusting its weights based on the optimizer and loss function.\n",
    "Banking Scenario: Training on Fraud Detection Data\n",
    "\n",
    "# Example training data: X_train is the feature matrix, y_train are the labels (0 for non-fraud, 1 for fraud)\n",
    "X_train = ...  # Transaction features\n",
    "y_train = ...  # Labels (0 = non-fraud, 1 = fraud)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab61a1d1-c11d-4116-a683-cbbc90fa41dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banking Scenario: Evaluating Fraud Detection Model\n",
    "\n",
    "# Example test data: X_test and y_test\n",
    "X_test = ...  # Test features\n",
    "y_test = ...  # Test labels\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a47e2a-89af-4a8f-90a8-c3ff100d8c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banking Scenario: Predicting Fraudulent Transactions\n",
    "\n",
    "# New transaction data (feature matrix for a single transaction)\n",
    "new_transaction = ...\n",
    "\n",
    "# Make a prediction\n",
    "prediction = model.predict(new_transaction)\n",
    "\n",
    "# If the output is greater than 0.5, classify as fraud\n",
    "if prediction[0] > 0.5:\n",
    "    print(\"Fraudulent transaction detected!\")\n",
    "else:\n",
    "    print(\"Transaction is legitimate.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8491a7b1-0076-4580-85c8-60d49cbb4d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banking Scenario: Saving and Loading Fraud Detection Model\n",
    "\n",
    "# Save the trained model to a file\n",
    "model.save(\"fraud_detection_model.h5\")\n",
    "\n",
    "# Later, load the saved model\n",
    "from keras.models import load_model\n",
    "loaded_model = load_model(\"fraud_detection_model.h5\")\n",
    "\n",
    "# Make predictions with the loaded model\n",
    "loaded_model.predict(new_transaction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe683d2-aef2-4f63-9c39-73c215f23373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Banking Scenario: Using Callbacks for Early Stopping\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Train the model with the callback\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fd26c9-24fa-4b77-940b-26469b347a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Code Sample:\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers\n",
    "model.add(Dense(units=64, input_dim=10, activation='relu'))  # input_dim is the number of features\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))  # Binary classification for credit risk (good/bad)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model (X_train, y_train would be your training data and labels)\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42799f4d-c423-4cbb-878f-3fcfed1a78ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional API Model\n",
    "# The Functional API is more flexible than the Sequential model. It allows you to create models where layers can have \n",
    "# multiple inputs and outputs, and you can share layers between models.\n",
    "# Use Case in Banking:\n",
    "# \tFraud Detection: Build a model that takes multiple types of data (transaction history, customer details) as inputs and predicts\n",
    "# fraudulent transactions.\n",
    "# # Python Code Sample:\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "# Define input layers\n",
    "input_transaction = Input(shape=(10,))  # 10 features for transaction data\n",
    "input_customer = Input(shape=(5,))      # 5 features for customer data\n",
    "\n",
    "# Process the inputs\n",
    "x = Dense(64, activation='relu')(input_transaction)\n",
    "y = Dense(64, activation='relu')(input_customer)\n",
    "\n",
    "# Merge the processed inputs\n",
    "merged = concatenate([x, y])\n",
    "\n",
    "# Output layer\n",
    "output = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[input_transaction, input_customer], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model (X_transaction, X_customer, y_train would be your training data)\n",
    "model.fit([X_transaction, X_customer], y_train, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2261fb87-6475-41dc-be95-db71bdc6c956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Subclassing\n",
    "# Model Subclassing is a more flexible approach where you can define your own model class by subclassing the\n",
    "# Keras Model class. This allows for complete control over the forward pass.\n",
    "# Use Case in Banking:\n",
    "# \tLoan Default Prediction: A complex model where you need fine-grained control over how layers are connected to predict loan defaults.\n",
    "# # Python Code Sample:\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras import layers\n",
    "\n",
    "class CustomModel(Model):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.dense1 = Dense(64, activation='relu')\n",
    "        self.dense2 = Dense(32, activation='relu')\n",
    "        self.dense3 = Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        return self.dense3(x)\n",
    "\n",
    "# Instantiate the model\n",
    "model = CustomModel()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model (X_train, y_train would be your training data)\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "# 4. Convolutional Neural Networks (CNN)\n",
    "# CNNs are primarily used for image-related tasks but can also be used for structured data if reformatted appropriately. \n",
    "# They use convolutional layers to extract features from data.\n",
    "# Use Case in Banking:\n",
    "# \tCheque Image Recognition: Use CNNs to extract features from cheque images (e.g., handwritten digits, signatures) for automatic cheque processing.\n",
    "# # Python Code Sample:\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Create a Sequential CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Add convolutional layers\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(64, 64, 3)))  # Input shape for images\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the output and add dense layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Binary classification (fraud/no fraud)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model (X_train and y_train would be your image data and labels)\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "# 5. Recurrent Neural Networks (RNN)\n",
    "# RNNs are suitable for sequential data. They can maintain an internal state that makes them useful for tasks like time series prediction.\n",
    "# Use Case in Banking: \tTime Series Forecasting: Predict future stock prices or interest rates based on historical data using RNNs.\n",
    "# # Python Code Sample:\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense\n",
    "\n",
    "# Create a Sequential model with RNN layers\n",
    "model = Sequential()\n",
    "\n",
    "# Add RNN layer\n",
    "model.add(SimpleRNN(50, input_shape=(10, 1), activation='relu'))  # 10 time steps, 1 feature\n",
    "\n",
    "# Add a dense output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Fit the model (X_train and y_train are your time-series data)\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "# 6. Long Short-Term Memory (LSTM)\n",
    "# LSTM is a type of RNN designed to learn long-term dependencies, which is useful when the data has long-range dependencies.\n",
    "# Use Case in Banking:\tCustomer Churn Prediction: Predict if a customer will churn based on their behavior over time, using LSTM for sequence data.\n",
    "# # Python Code Sample:\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# Create a Sequential model with LSTM layers\n",
    "model = Sequential()\n",
    "\n",
    "# Add LSTM layer\n",
    "model.add(LSTM(50, input_shape=(10, 1), activation='relu', return_sequences=False))\n",
    "\n",
    "# Add a dense output layer\n",
    "model.add(Dense(1, activation='sigmoid'))  # Binary classification for churn prediction\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model (X_train and y_train are your sequential data)\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "# 7. Autoencoders\n",
    "# Autoencoders are unsupervised learning models that learn to encode input data into a smaller latent representation and then \n",
    "# reconstruct the data back to its original form.\n",
    "# Use Case in Banking:\n",
    "# \tAnomaly Detection: Detect fraudulent transactions by learning a reconstruction of normal transactions and flagging any \n",
    "# transaction that doesnt fit.\n",
    "# # Python Code Sample:\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create an autoencoder model\n",
    "model = Sequential()\n",
    "\n",
    "# Encoder\n",
    "model.add(Dense(128, activation='relu', input_dim=10))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Latent space representation\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Decoder\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='sigmoid'))  # Same number of output neurons as input\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Fit the model (X_train would be your data)\n",
    "model.fit(X_train, X_train, epochs=10, batch_size=32)\n",
    "# 8. Generative Adversarial Networks (GANs)\n",
    "# GANs consist of two models: a generator and a discriminator. The generator creates fake data, and the discriminator \n",
    "# attempts to distinguish between real and fake data.\n",
    "# Use Case in Banking:\n",
    "# \tSynthetic Data Generation: Use GANs to generate synthetic transaction data for training fraud detection models when real data is scarce.\n",
    "# # Python Code Sample:\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "\n",
    "# Define the generator model\n",
    "generator = Sequential()\n",
    "generator.add(Dense(128, input_dim=100, activation='relu'))\n",
    "generator.add(Dense(256, activation='relu'))\n",
    "generator.add(Dense(512, activation='relu'))\n",
    "generator.add(Dense(1024, activation='relu'))\n",
    "generator.add(Dense(10, activation='sigmoid'))  # Output synthetic data\n",
    "\n",
    "# Define the discriminator model\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Dense(1024, input_dim=10, activation='relu'))\n",
    "discriminator.add(Dense(512, activation='relu'))\n",
    "discriminator.add(Dense(256, activation='relu'))\n",
    "discriminator.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
    "\n",
    "# Compile the models\n",
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# GAN model combines the generator and discriminator\n",
    "discriminator.trainable = False\n",
    "gan_input = Input(shape=(100,))\n",
    "x = generator(gan_input)\n",
    "gan_output = discriminator(x)\n",
    "gan_model = Model(gan_input, gan_output)\n",
    "gan_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695b37e4-35db-4035-9b32-00deb22f5e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business Scenario: Credit Scoring: A feed-forward neural network can be used to predict whether a loan applicant will \n",
    "# default or not, based on their financial information (e.g., income, loan amount, credit history, etc.).\n",
    "# Python Code Example:\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "# Sample data (e.g., 1000 customers, each with 5 features like age, income, loan amount, etc.)\n",
    "X_train = np.random.rand(1000, 5)  # 1000 customers, 5 features\n",
    "y_train = np.random.randint(0, 2, 1000)  # 0 = no default, 1 = default\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer and first hidden layer\n",
    "model.add(Dense(units=64, input_dim=5, activation='relu'))  # 5 input features\n",
    "\n",
    "# Second hidden layer\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "\n",
    "# Output layer (binary classification)\n",
    "model.add(Dense(units=1, activation='sigmoid'))  # Sigmoid for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Predict on new data (new applicants)\n",
    "new_data = np.random.rand(1, 5)  # A new applicant with 5 features\n",
    "prediction = model.predict(new_data)\n",
    "print(\"Default Probability: \", prediction)\n",
    "# 2. Convolutional Neural Networks (CNN)\n",
    "# Convolutional Neural Networks (CNNs) are used for image-based tasks but can also be applied to certain types of \n",
    "# structured data, like time-series or sequence data, in a reformatted manner.\n",
    "# Business Scenario: Cheque Image Recognition\n",
    "# In banks, cheque processing can be automated by using CNNs to recognize handwritten digits, signatures, and text from cheque images.\n",
    "# Python Code Example:\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Assuming we have preprocessed and reshaped image data\n",
    "# Let's assume the images are 64x64 pixels with 1 channel (grayscale)\n",
    "X_train_images = np.random.rand(1000, 64, 64, 1)  # 1000 images of size 64x64\n",
    "y_train_labels = np.random.randint(0, 2, 1000)  # 0 = fake cheque, 1 = real cheque\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Add Convolutional Layer\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(64, 64, 1)))\n",
    "\n",
    "# Add MaxPooling Layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the output from the convolutional layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add Fully Connected (Dense) Layers\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_images, y_train_labels, epochs=10, batch_size=32)\n",
    "\n",
    "# Predict on new images\n",
    "new_image = np.random.rand(1, 64, 64, 1)  # A new cheque image\n",
    "prediction = model.predict(new_image)\n",
    "print(\"Cheque authenticity prediction: \", prediction)\n",
    "# 3. Recurrent Neural Networks (RNN)\n",
    "# Recurrent Neural Networks (RNNs) are designed for sequential data. They are useful for time series prediction and other tasks \n",
    "# where the sequence of data is important, such as analyzing customer behavior over time.\n",
    "# Business Scenario: Customer Churn Prediction\n",
    "# A bank may want to predict whether a customer will churn (leave) based on their past behavior, such as transaction history \n",
    "# and interaction with customer support.\n",
    "# Python Code Example:\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense\n",
    "import numpy as np\n",
    "\n",
    "# Sample data (e.g., 1000 customers with 10 time-steps of transaction history)\n",
    "X_train = np.random.rand(1000, 10, 1)  # 10 time-steps, 1 feature (transaction amount)\n",
    "y_train = np.random.randint(0, 2, 1000)  # 0 = no churn, 1 = churn\n",
    "\n",
    "# Define the RNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Add SimpleRNN layer\n",
    "model.add(SimpleRNN(50, input_shape=(10, 1), activation='relu'))  # 10 time-steps, 1 feature\n",
    "\n",
    "# Add a Dense output layer\n",
    "model.add(Dense(1, activation='sigmoid'))  # Binary classification (churn or not)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Predict on new customer data\n",
    "new_data = np.random.rand(1, 10, 1)  # New customer transaction data\n",
    "prediction = model.predict(new_data)\n",
    "print(\"Churn prediction: \", prediction)\n",
    "# 4. Long Short-Term Memory (LSTM) Networks\n",
    "# LSTM is a special type of RNN designed to capture long-term dependencies in sequential data. It is especially useful for datasets\n",
    "# where long-range dependencies are critical.\n",
    "# Business Scenario: Predicting Stock Prices\n",
    "# Banks and financial institutions often use LSTM models to forecast stock prices, interest rates, or foreign exchange rates based on historical data.\n",
    "# Python Code Example:\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "# Sample data (e.g., 1000 days of stock price data)\n",
    "X_train = np.random.rand(1000, 60, 1)  # 60 time-steps (days), 1 feature (price)\n",
    "y_train = np.random.rand(1000)  # Next day's price prediction\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "# Add LSTM layer\n",
    "model.add(LSTM(50, input_shape=(60, 1), activation='relu'))\n",
    "\n",
    "# Add Dense output layer\n",
    "model.add(Dense(1))  # Predicting the next day's price (continuous output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Predict on new stock price data\n",
    "new_data = np.random.rand(1, 60, 1)  # New 60 days of stock data\n",
    "prediction = model.predict(new_data)\n",
    "print(\"Next day's predicted stock price: \", prediction)\n",
    "# 5. Autoencoders\n",
    "# Autoencoders are unsupervised neural networks used for data compression and anomaly detection. They learn to compress data into \n",
    "# a lower-dimensional representation and then reconstruct it.\n",
    "# Business Scenario: Anomaly Detection in Transactions\n",
    "# Banks can use autoencoders to detect unusual transactions that might indicate fraud. Transactions that cannot be reconstructed \n",
    "# well by the autoencoder are flagged as potential anomalies.\n",
    "# Python Code Example:\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "# Sample data (e.g., 1000 transaction records with 10 features)\n",
    "X_train = np.random.rand(1000, 10)  # 1000 transactions, 10 features (amount, location, time, etc.)\n",
    "\n",
    "# Define the autoencoder model\n",
    "model = Sequential()\n",
    "\n",
    "# Encoder\n",
    "model.add(Dense(8, activation='relu', input_dim=10))  # Compress from 10 features to 8\n",
    "model.add(Dense(4, activation='relu'))  # Further compression to 4\n",
    "\n",
    "# Decoder\n",
    "model.add(Dense(8, activation='relu'))  # Expanding back to 8\n",
    "model.add(Dense(10, activation='sigmoid'))  # Reconstructing back to 10 features\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, X_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Predict on new data\n",
    "new_transaction = np.random.rand(1, 10)  # A new transaction record\n",
    "reconstructed = model.predict(new_transaction)\n",
    "\n",
    "# Compare the reconstructed data to the original data\n",
    "print(\"Reconstructed transaction data: \", reconstructed)\n",
    "# 6. Generative Adversarial Networks (GANs)\n",
    "# Generative Adversarial Networks (GANs) are composed of two models: a generator that creates fake data and a discriminator \n",
    "# that tries to distinguish between real and fake data. GANs are typically used for generating synthetic data.\n",
    "# Business Scenario: Synthetic Fraudulent Transaction Generation\n",
    "# Banks can use GANs to generate synthetic fraudulent transaction data to train fraud detection models, especially when real \n",
    "# fraudulent data is scarce.\n",
    "# Python Code Example:\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "# Define the generator model (generates fake data)\n",
    "generator = Sequential()\n",
    "generator.add(Dense(128, input_dim=100, activation='relu'))\n",
    "generator.add(Dense(256, activation='relu'))\n",
    "generator.add(Dense(512, activation='relu'))\n",
    "generator.add(Dense(1024, activation='relu'))\n",
    "generator.add(Dense(10, activation='sigmoid'))  # 10 features (synthetic transaction)\n",
    "\n",
    "# Define the discriminator model (classifies real vs fake data)\n",
    "discriminator = Sequential()\n",
    "discriminator.add(Dense(1024, input_dim=10, activation='relu'))\n",
    "discriminator.add(Dense(512, activation='relu'))\n",
    "discriminator.add(Dense(256, activation='relu'))\n",
    "discriminator.add(Dense(1, activation='sigmoid'))  # Binary classification (real or fake)\n",
    "\n",
    "# Compile the discriminator model\n",
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# GAN model combines generator and discriminator\n",
    "discriminator.trainable = False\n",
    "gan_input = Input(shape=(100,))\n",
    "x = generator(gan_input)\n",
    "gan_output = discriminator(x)\n",
    "gan_model = Model(gan_input, gan_output)\n",
    "gan_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
